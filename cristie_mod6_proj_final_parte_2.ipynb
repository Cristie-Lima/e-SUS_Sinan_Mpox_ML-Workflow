{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Cristie-Lima/e-SUS_Sinan_Mpox_ML-Workflow/blob/main/cristie_mod6_proj_final_parte_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "---\n",
        "# **Ci√™ncia de Dados: Fundamentos de Machine Learning**\n",
        "---\n",
        "---"
      ],
      "metadata": {
        "id": "UZ4Wa0NV26dp"
      },
      "id": "UZ4Wa0NV26dp"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xdL1sKgB26dp"
      },
      "source": [
        "## **P√≥s-gradua√ß√£o em Ci√™ncia de Dados (2025/2026)**\n",
        "\n",
        "**Escola Superior de Tecnologia da Universidade Estadual do Amazonas - EST/UEA**\n",
        "    \n",
        "**Disciplina:** Fundamentos de Machine Learning\n",
        "\n",
        "**Prof. Me.:** Mario Bessa\n",
        "\n",
        "_**Reposit√≥rio do Projeto Acad√™mico no Github:** [e-SUS_Sinan_Mpox_ML-Workflow](https://github.com/Cristie-Lima/e-SUS_Sinan_Mpox_ML-Workflow)_\n",
        "\n",
        "_**Alunos:**_\n",
        "\n",
        "- _A. Cristiane R. Lima (Cristie)_\n",
        "- _Jos√© Henrique Santos Cavalcante (Henrique)_\n",
        "\n",
        "_**Data:** 30 de agosto de 2025_\n"
      ],
      "id": "xdL1sKgB26dp"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7itil0XZ26dq"
      },
      "source": [
        "## **Projeto Final**\n",
        "Este projeto est√° dividido em 2 notebooks para contemplar o fluxo completo de pr√©-modelagem e modelagem para o dataset [e-SUS Sinan/Mpox](https://opendatasus.saude.gov.br/ne/dataset/mpox), seguindo uma estrutura padronizada, do seguinte modo:\n",
        "\n",
        "- cristie_mod6_proj_final_parte_1.ipynb: Aquisi√ß√£o, An√°lise Descritiva e Explorat√≥ria de Dados (EDA), Pr√©-processamento (Corre√ß√£o )\n",
        "\n",
        "Foi desenvolvido a partir do notebook-base apresentado e explicado ao longo as aulas te√≥rico-pr√°ticas em laborat√≥rio do Professor.\n",
        "\n",
        "---\n",
        "\n",
        "üí° **Nota t√©cnica:** O link para o Dicion√°rio de Dados est√° dispon√≠vel na se√ß√£o de *An√°lise Descritiva*. Recomenda-se mant√™-lo aberto para facilitar a interpreta√ß√£o das vari√°veis.\n",
        "\n",
        "‚ö†Ô∏è **Observa√ß√£o:** Linhas com o coment√°rio `#debug#` indicam trechos opcionais para testes e inspe√ß√µes pontuais ‚Äî como a gera√ß√£o de arquivos `.csv` tempor√°rios para verificar o estado do DataFrame ap√≥s imputa√ß√µes.\n",
        "\n",
        "üìö **Refer√™ncia:**  \n",
        "KELLEHER, John D.; MAC NAMEE, Brian; D‚ÄôARCY, Aoife. *Fundamentals of Machine Learning for Predictive Data Analytics: Algorithms, Worked Examples, and Case Studies*. 2. ed. Cambridge, MA: MIT Press, 2020.\n"
      ],
      "id": "7itil0XZ26dq"
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "### **Contextualiza√ß√£o do Projeto**\n",
        "\n",
        "A Mpox (Monkeypox) √© uma doen√ßa infecciosa emergente que ganhou aten√ß√£o internacional devido ao seu potencial de dissemina√ß√£o e impacto na sa√∫de p√∫blica. No Brasil, o sistema e-SUS Sinan tem sido fundamental para o registro e monitoramento dos casos, permitindo a constru√ß√£o de bases de dados estruturadas para an√°lise epidemiol√≥gica.\n",
        "\n",
        "Este projeto tem como objetivo aplicar t√©cnicas de aprendizado de m√°quina para explorar, tratar e modelar os dados dispon√≠veis, com foco na gera√ß√£o de insights preditivos que possam apoiar estrat√©gias de vigil√¢ncia e resposta. A abordagem adotada contempla desde o pr√©-processamento e imputa√ß√£o de dados at√© a constru√ß√£o de modelos supervisionados, seguindo diretrizes metodol√≥gicas discutidas em ambiente acad√™mico.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "pJXC5DR226dq"
      },
      "id": "pJXC5DR226dq"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Instru√ß√µes:**\n",
        "\n",
        "- Realizar os processamentos abaixo:\n",
        "\n",
        "  - No notebook `cristie_mod6_proj_final_parte_1.ipynb`:\n",
        "    -  Atividade 1: Limpar dados incorretos\n",
        "    -  Atividade 2: Imputa√ß√£o de valores faltantes\n",
        "  - No notebook `cristie_mod6_proj_final_parte_2.ipynb`:\n",
        "    -  Atividade 3: Codifica√ß√£o de vari√°veis categ√≥ricas (OrdinalEncoder e OneHotEncoder)\n",
        "    -  Atividade 4: Escalonamento de vari√°veis num√©ricas (StandardScaler e MinMaxScaler)\n",
        "    -  Atividade 5: Balanceamento dos dados (Tomek e Smote)\n",
        "    -  Atividade 6: Treinamento dos modelos de Machine Learning\n",
        "    -  Atividade 7: Usar o modelo treinado"
      ],
      "metadata": {
        "id": "ivP80HuL26dq"
      },
      "id": "ivP80HuL26dq"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Importa bibliotecas"
      ],
      "metadata": {
        "id": "1zpmIGn2KjEU"
      },
      "id": "1zpmIGn2KjEU"
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "\n",
        "# Para filtro de vari√¢ncia\n",
        "from sklearn.feature_selection import VarianceThreshold\n",
        "\n",
        "\n",
        "# Para garantir X 100% num√©rico e sem NaN antes de Tomek/SMOTE\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "# Para aplicar TomerkLinks e SMOTE\n",
        "from imblearn.under_sampling import TomekLinks\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "#import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "0GU1WqNWKnSu"
      },
      "id": "0GU1WqNWKnSu",
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tarefas preliminares"
      ],
      "metadata": {
        "id": "hp4HG5TkPkxM"
      },
      "id": "hp4HG5TkPkxM"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Montar o Google Drive no Colab"
      ],
      "metadata": {
        "id": "tca799qUKc9w"
      },
      "id": "tca799qUKc9w"
    },
    {
      "cell_type": "code",
      "source": [
        "#Montar o Google Drive no Colab\n",
        "#from google.colab import drive\n",
        "\n",
        "# Montar o Google Drive na pasta /content/drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HgH0Aln6LkoU",
        "outputId": "30d47051-cda7-47fa-c81e-d71c7aa3c87e"
      },
      "id": "HgH0Aln6LkoU",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Registra o caminho da pasta do projeto"
      ],
      "metadata": {
        "id": "cnJREKnUP0VN"
      },
      "id": "cnJREKnUP0VN"
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Caminho da pasta do projeto (ajuste conforme sua organiza√ß√£o)\n",
        "project_path = \"/content/drive/MyDrive/data_mpox_2022_Part1/\"\n",
        "\n",
        "# Conferir se a pasta existe, sen√£o criar\n",
        "import os\n",
        "os.makedirs(project_path, exist_ok=True)\n",
        "\n",
        "print(\"‚úÖ Drive montado em:\", project_path)\n"
      ],
      "metadata": {
        "id": "Qeq7TnEdKZuK"
      },
      "id": "Qeq7TnEdKZuK",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üìÇ Desserializa√ß√£o do dataset pr√©-processado"
      ],
      "metadata": {
        "id": "9m67S9fAKAsC"
      },
      "id": "9m67S9fAKAsC"
    },
    {
      "cell_type": "code",
      "source": [
        "# üìÇ Desserializa√ß√£o do dataset pr√©-processado\n",
        "\n",
        "filename = project_path + \"mpox_2022_prep.parquet\"\n",
        "df_prep = pd.read_parquet(filename)\n",
        "print(\"‚úÖ df_prep carregado com sucesso:\", df_prep.shape)\n",
        "\n",
        "# Visualizar primeiras linhas\n",
        "df_prep.head(1)"
      ],
      "metadata": {
        "id": "8BNAEcNMJ-op"
      },
      "id": "8BNAEcNMJ-op",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "## **‚öôÔ∏è Engenharia de Atributos: criando novas vari√°veis a partir das existentes.**\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "    Resumo da a√ß√µes:\n",
        "      1. Explos√£o de SINTOMA ‚Üí dummies (min_freq opcional; default preserva todos).\n",
        "      2. Deltas de datas (sem imputar; respeita NaT).\n",
        "      3. Agregados cl√≠nicos (ISTs, imuno, vacina).\n",
        "      4. Bins demogr√°ficos (idade, regi√£o).\n",
        "      5. Intera√ß√µes (ex.: imuno √ó sintomas, centraliza√ß√£o de delay por regi√£o).\n",
        "      6. Targets (hosp, √≥bito).\n",
        "      7. Auditorias.\n",
        "\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "tdoF2Scm6iWF"
      },
      "id": "tdoF2Scm6iWF"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Engenharia de Atributos ‚Äî Fun√ß√µes"
      ],
      "metadata": {
        "id": "BprpIlrrDm5y"
      },
      "id": "BprpIlrrDm5y"
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# ‚öôÔ∏è ENGENHARIA DE ATRIBUTOS ‚Äî FUN√á√ïES E EXECU√á√ÉO\n",
        "# ==========================================\n",
        "#import pandas as pd\n",
        "#import numpy as np\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 1) Explos√£o de sintomas em dummies (com min_freq opcional)\n",
        "# ------------------------------------------------------------\n",
        "def explode_sintoma_dummies_simple(\n",
        "    df: pd.DataFrame,\n",
        "    col: str = \"SINTOMA\",\n",
        "    prefix: str = \"sx_\",\n",
        "    min_freq: int | None = None\n",
        ") -> tuple[pd.DataFrame, list]:\n",
        "    \"\"\"\n",
        "    Transforma a coluna de sintomas (strings separadas por v√≠rgula) em colunas dummies (0/1).\n",
        "\n",
        "    Par√¢metros\n",
        "    ----------\n",
        "    df : DataFrame\n",
        "        Base de entrada.\n",
        "    col : str\n",
        "        Nome da coluna que cont√©m a lista de sintomas (ex.: \"Febre, Cefaleia\").\n",
        "    prefix : str\n",
        "        Prefixo a ser usado nas colunas dummies geradas (ex.: \"sx_\").\n",
        "    min_freq : int | None\n",
        "        - None ‚Üí mant√©m TODOS os sintomas (recomendado para sa√∫de; preserva informa√ß√£o).\n",
        "        - inteiro ‚â•1 ‚Üí mant√©m apenas sintomas com frequ√™ncia absoluta ‚â• min_freq\n",
        "                       (√∫til futuramente para reduzir esparsidade).\n",
        "\n",
        "    Retorno\n",
        "    -------\n",
        "    df_out : DataFrame\n",
        "        DataFrame original + colunas dummies para cada sintoma selecionado.\n",
        "    sintomas_cols : list[str]\n",
        "        Lista com os nomes das colunas de sintomas criadas.\n",
        "    \"\"\"\n",
        "    df_out = df.copy()\n",
        "\n",
        "    if col not in df_out.columns:\n",
        "        return df_out, []\n",
        "\n",
        "    # Lista de sintomas por linha (min√∫sculas, trim de espa√ßos; ignora strings vazias)\n",
        "    s_clean = df_out[col].fillna(\"\").astype(str)\n",
        "    listas = s_clean.str.split(\",\").apply(lambda xs: [t.strip().lower() for t in xs if t.strip() != \"\"])\n",
        "\n",
        "    # Frequ√™ncia dos termos\n",
        "    freq = pd.Series([t for sub in listas for t in sub]).value_counts()\n",
        "\n",
        "    # Sele√ß√£o por min_freq (se None ‚Üí todos)\n",
        "    if min_freq is None:\n",
        "        termos = freq.index\n",
        "    else:\n",
        "        termos = freq[freq >= int(min_freq)].index\n",
        "\n",
        "    # Cria√ß√£o de dummies (0/1) linha a linha\n",
        "    for termo in termos:\n",
        "        c = f\"{prefix}{termo.replace(' ', '_')}\"\n",
        "        df_out[c] = listas.apply(lambda L: int(termo in L)).astype(\"Int64\")\n",
        "\n",
        "    sintomas_cols = [c for c in df_out.columns if c.startswith(prefix)]\n",
        "    return df_out, sintomas_cols\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 2) Features derivadas de datas (sem imputar datas)\n",
        "# ------------------------------------------------------------\n",
        "def add_date_features(\n",
        "    df: pd.DataFrame,\n",
        "    col_notif: str = \"DT_NOTIFIC\",\n",
        "    col_inicio: str = \"DT_SIN_PRI\",\n",
        "    col_evol: str = \"DT_EVOLUCAO\",\n",
        "    col_intern: str = \"DT_INTERNA\",\n",
        "    create_missing_flags: bool = True\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Cria deltas de tempo (em dias) a partir de colunas de datas ‚Äî sem imputa√ß√£o.\n",
        "    Se a linha n√£o tiver ambas as datas v√°lidas, o delta fica NaN.\n",
        "\n",
        "    Par√¢metros\n",
        "    ----------\n",
        "    df : DataFrame\n",
        "        Base de entrada (datas podem estar em string; ser√£o convertidas durante o c√°lculo).\n",
        "    col_notif, col_inicio, col_evol, col_intern : str\n",
        "        Nomes das colunas de datas conforme o dicion√°rio.\n",
        "    create_missing_flags : bool\n",
        "        Se True, mant√©m/gera indicadores *_missing (0/1) para cada coluna de data original.\n",
        "\n",
        "    Retorno\n",
        "    -------\n",
        "    DataFrame com as colunas:\n",
        "      - delay_notif        = DT_NOTIFIC - DT_SIN_PRI\n",
        "      - tempo_internacao   = DT_EVOLUCAO - DT_INTERNA\n",
        "      - duracao_caso       = DT_EVOLUCAO - DT_SIN_PRI\n",
        "    \"\"\"\n",
        "    out = df.copy()\n",
        "\n",
        "    # Flags de aus√™ncia (se existirem as colunas)\n",
        "    def _flag(col):\n",
        "        if create_missing_flags and col in out.columns:\n",
        "            out[f\"{col}_missing\"] = out[col].isna().astype(\"Int64\")\n",
        "\n",
        "    for c in [col_notif, col_inicio, col_evol, col_intern]:\n",
        "        _flag(c)\n",
        "\n",
        "    # Converte sob demanda apenas para calcular deltas\n",
        "    to_dt = lambda s: pd.to_datetime(s, errors=\"coerce\", dayfirst=True)\n",
        "\n",
        "    if {col_notif, col_inicio}.issubset(out.columns):\n",
        "        out[\"delay_notif\"] = (to_dt(out[col_notif]) - to_dt(out[col_inicio])).dt.days\n",
        "\n",
        "    if {col_evol, col_intern}.issubset(out.columns):\n",
        "        out[\"tempo_internacao\"] = (to_dt(out[col_evol]) - to_dt(out[col_intern])).dt.days\n",
        "\n",
        "    if {col_evol, col_inicio}.issubset(out.columns):\n",
        "        out[\"duracao_caso\"] = (to_dt(out[col_evol]) - to_dt(out[col_inicio])).dt.days\n",
        "\n",
        "    return out\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 3) Agregados cl√≠nicos (ISTs, imunossupress√£o, vacina√ß√£o)\n",
        "# ------------------------------------------------------------\n",
        "def add_clinical_aggregates(\n",
        "    df: pd.DataFrame,\n",
        "    ist_cols: list | None = None,\n",
        "    col_imuno: str = \"PAC_IMUNOSSUP\",\n",
        "    col_vacina: str = \"VACINA\"\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Cria agrega√ß√µes cl√≠nicas:\n",
        "\n",
        "    - ist_any   : 1 se pelo menos uma IST espec√≠fica = 1; 0 caso contr√°rio (ignora NaN ao somar/max).\n",
        "    - ist_count : soma das ISTs espec√≠ficas (contagem).\n",
        "    - imune_risco:\n",
        "        PAC_IMUNOSSUP (1,2,3) ‚Üí 1; 4‚Üí0; outros (9/NaN) ‚Üí NA.\n",
        "    - vacina_qualquer:\n",
        "        VACINA in {1,2,3} ‚Üí 1; VACINA==4 ‚Üí 0; caso contr√°rio ‚Üí NA.\n",
        "\n",
        "    Par√¢metros\n",
        "    ----------\n",
        "    df : DataFrame\n",
        "    ist_cols : list | None\n",
        "        Lista expl√≠cita de colunas de ISTs. Se None, usa o conjunto padr√£o do mpox_2022.\n",
        "    col_imuno : str\n",
        "        Nome da coluna de imunossupress√£o.\n",
        "    col_vacina : str\n",
        "        Nome da coluna de vacina√ß√£o.\n",
        "\n",
        "    Retorno\n",
        "    -------\n",
        "    DataFrame com novas colunas: ist_any, ist_count, imune_risco, vacina_qualquer.\n",
        "    \"\"\"\n",
        "    out = df.copy()\n",
        "\n",
        "    if ist_cols is None:\n",
        "        ist_cols = [\n",
        "            \"GONORREIA\",\"CLAMIDEA\",\"SIFILIS\",\"HERPES_GENITAL\",\"CANCRO_MOLE\",\n",
        "            \"TRICHOMOMAS_VAGINALS\",\"LINFOGRANULOMA\",\"MYCOPLASMA_GENITAL\",\n",
        "            \"HPV\",\"DIP\",\"DONOVANOSE\",\"HTLV\",\"VERRUGA_GENITAL\"\n",
        "        ]\n",
        "    ist_cols = [c for c in ist_cols if c in out.columns]\n",
        "\n",
        "    if ist_cols:\n",
        "        out[\"ist_any\"] = out[ist_cols].max(axis=1, skipna=True).astype(\"Int64\")\n",
        "        out[\"ist_count\"] = out[ist_cols].sum(axis=1, skipna=True).astype(\"Int64\")\n",
        "\n",
        "    if col_imuno in out.columns:\n",
        "        m = out[col_imuno].map({1:1, 2:1, 3:1, 4:0})\n",
        "        out[\"imune_risco\"] = m.astype(\"Int64\")\n",
        "\n",
        "    if col_vacina in out.columns:\n",
        "        v = pd.Series(pd.NA, index=out.index, dtype=\"Int64\")\n",
        "        v = v.mask(out[col_vacina].isin([1,2,3])==True, 1)\n",
        "        v = v.mask(out[col_vacina].eq(4)==True, 0)\n",
        "        out[\"vacina_qualquer\"] = v\n",
        "\n",
        "    return out\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 4) Bins demogr√°ficos / Regi√£o (UF‚Üíregi√£o)\n",
        "# ------------------------------------------------------------\n",
        "def add_demo_bins(\n",
        "    df: pd.DataFrame,\n",
        "    col_idade: str = \"NU_IDADE_N\",\n",
        "    col_uf: str = \"SG_UF\"\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Cria faixas et√°rias e regi√£o (a partir de SG_UF).\n",
        "    - faixa_etaria: categorias textuais (0‚Äì9, 10‚Äì17, 18‚Äì29, 30‚Äì39, 40‚Äì49, 50‚Äì59, 60+).\n",
        "      (Mant√©m NaN quando NU_IDADE_N ausente).\n",
        "    - regiao: Norte, Nordeste, Centro-Oeste, Sudeste, Sul (NaN se SG_UF ausente ou desconhecido).\n",
        "\n",
        "    Par√¢metros\n",
        "    ----------\n",
        "    df : DataFrame\n",
        "    col_idade : str\n",
        "        Nome da coluna de idade (anos).\n",
        "    col_uf : str\n",
        "        Nome da coluna de UF (sigla).\n",
        "\n",
        "    Retorno\n",
        "    -------\n",
        "    DataFrame com 'faixa_etaria' (category) e 'regiao' (category).\n",
        "    \"\"\"\n",
        "    out = df.copy()\n",
        "\n",
        "    # Faixas et√°rias (ajust√°veis)\n",
        "    if col_idade in out.columns:\n",
        "        bins = [-np.inf, 9, 17, 29, 39, 49, 59, np.inf]\n",
        "        labels = [\"0-9\",\"10-17\",\"18-29\",\"30-39\",\"40-49\",\"50-59\",\"60+\"]\n",
        "        out[\"faixa_etaria\"] = pd.cut(out[col_idade], bins=bins, labels=labels).astype(\"category\")\n",
        "\n",
        "    # Mapeamento UF ‚Üí Regi√£o (IBGE)\n",
        "    uf_regiao = {\n",
        "        # Norte\n",
        "        \"AC\":\"Norte\",\"AP\":\"Norte\",\"AM\":\"Norte\",\"PA\":\"Norte\",\"RO\":\"Norte\",\"RR\":\"Norte\",\"TO\":\"Norte\",\n",
        "        # Nordeste\n",
        "        \"AL\":\"Nordeste\",\"BA\":\"Nordeste\",\"CE\":\"Nordeste\",\"MA\":\"Nordeste\",\"PB\":\"Nordeste\",\n",
        "        \"PE\":\"Nordeste\",\"PI\":\"Nordeste\",\"RN\":\"Nordeste\",\"SE\":\"Nordeste\",\n",
        "        # Centro-Oeste\n",
        "        \"DF\":\"Centro-Oeste\",\"GO\":\"Centro-Oeste\",\"MS\":\"Centro-Oeste\",\"MT\":\"Centro-Oeste\",\n",
        "        # Sudeste\n",
        "        \"ES\":\"Sudeste\",\"MG\":\"Sudeste\",\"RJ\":\"Sudeste\",\"SP\":\"Sudeste\",\n",
        "        # Sul\n",
        "        \"PR\":\"Sul\",\"RS\":\"Sul\",\"SC\":\"Sul\",\n",
        "    }\n",
        "    if col_uf in out.columns:\n",
        "        out[\"regiao\"] = out[col_uf].map(uf_regiao).astype(\"category\")\n",
        "\n",
        "    return out\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 5) Intera√ß√µes (exemplos pragm√°ticos)\n",
        "# ------------------------------------------------------------\n",
        "def add_interactions(\n",
        "    df: pd.DataFrame,\n",
        "    sintomas_prefix: str = \"sx_\",\n",
        "    center_delay_by: str = \"regiao\"\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Cria intera√ß√µes/transforma√ß√µes simples:\n",
        "\n",
        "    - sintoma_count: soma dos dummies de sintomas (se existirem colunas com prefixo sintomas_prefix).\n",
        "    - imune_risco_x_sintomas: produto de imune_risco * sintoma_count (se ambas existirem).\n",
        "    - delay_notif_centered_regiao: delay_notif centrado pela m√©dia da regi√£o (se 'regiao' existir).\n",
        "\n",
        "    Par√¢metros\n",
        "    ----------\n",
        "    df : DataFrame\n",
        "    sintomas_prefix : str\n",
        "        Prefixo que identifica as colunas dummies de sintomas (default 'sx_').\n",
        "    center_delay_by : str\n",
        "        Coluna categ√≥rica para centralizar delay_notif por grupo (default 'regiao').\n",
        "\n",
        "    Retorno\n",
        "    -------\n",
        "    DataFrame com novas colunas se aplic√°veis.\n",
        "    \"\"\"\n",
        "    out = df.copy()\n",
        "\n",
        "    # Conta dummies de sintomas\n",
        "    sint_cols = [c for c in out.columns if c.startswith(sintomas_prefix)]\n",
        "    if sint_cols:\n",
        "        out[\"sintoma_count\"] = out[sint_cols].sum(axis=1, skipna=True).astype(\"Int64\")\n",
        "\n",
        "    # Intera√ß√£o com imunossupress√£o\n",
        "    if {\"imune_risco\",\"sintoma_count\"}.issubset(out.columns):\n",
        "        # Usa 0 para NA ao multiplicar; resultado volta a Int64 com NA onde ambos eram NA\n",
        "        tmp = out[\"imune_risco\"].fillna(0) * out[\"sintoma_count\"].fillna(0)\n",
        "        tmp = tmp.mask(out[\"imune_risco\"].isna() & out[\"sintoma_count\"].isna(), pd.NA)\n",
        "        out[\"imune_risco_x_sintomas\"] = tmp.astype(\"Int64\")\n",
        "\n",
        "    # Centraliza√ß√£o de delay_notif por regi√£o (ou outra coluna categ√≥rica)\n",
        "    if \"delay_notif\" in out.columns and center_delay_by in out.columns:\n",
        "        out[\"delay_notif_centered_regiao\"] = (\n",
        "            out[\"delay_notif\"] - out.groupby(center_delay_by)[\"delay_notif\"].transform(\"mean\")\n",
        "        )\n",
        "\n",
        "    return out\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 6) Targets (vari√°veis-alvo bin√°rias)\n",
        "# ------------------------------------------------------------\n",
        "def add_targets(\n",
        "    df: pd.DataFrame,\n",
        "    col_hosp: str = \"HOSPITAL\",\n",
        "    col_evol: str = \"EVOLUCAO\"\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Cria vari√°veis-alvo padronizadas para classifica√ß√£o:\n",
        "\n",
        "    - target_hosp      : 1 se HOSPITAL == 1; 0 caso contr√°rio (Int64 com NA onde n√£o h√° dado).\n",
        "    - target_obito_any : 1 se EVOLUCAO ‚àà {1 (√≥bito MPX), 3 (√≥bito outra causa)}; 0 caso contr√°rio.\n",
        "    - target_obito_mpx : 1 se EVOLUCAO == 1; 0 caso contr√°rio.\n",
        "\n",
        "    Par√¢metros\n",
        "    ----------\n",
        "    df : DataFrame\n",
        "    col_hosp : str\n",
        "        Nome da coluna de hospitaliza√ß√£o (c√≥digo).\n",
        "    col_evol : str\n",
        "        Nome da coluna de evolu√ß√£o (c√≥digo).\n",
        "\n",
        "    Retorno\n",
        "    -------\n",
        "    DataFrame com as tr√™s colunas de target.\n",
        "    \"\"\"\n",
        "    out = df.copy()\n",
        "\n",
        "    if col_hosp in out.columns:\n",
        "        t = out[col_hosp].eq(1)\n",
        "        out[\"target_hosp\"] = t.where(out[col_hosp].notna(), pd.NA).astype(\"Int64\")\n",
        "\n",
        "    if col_evol in out.columns:\n",
        "        any_obito = out[col_evol].isin([1,3])\n",
        "        out[\"target_obito_any\"] = any_obito.where(out[col_evol].notna(), pd.NA).astype(\"Int64\")\n",
        "        mpx = out[col_evol].eq(1)\n",
        "        out[\"target_obito_mpx\"] = mpx.where(out[col_evol].notna(), pd.NA).astype(\"Int64\")\n",
        "\n",
        "    return out\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 7) Auditorias\n",
        "# ------------------------------------------------------------\n",
        "def audit_sintomas(df: pd.DataFrame, sintomas_cols: list[str]) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Auditoria das dummies de sintomas:\n",
        "    - non_nulls, sum_positives, percent_positives\n",
        "    - status: marca colunas sempre-zero / sempre-NaN\n",
        "    \"\"\"\n",
        "    if not sintomas_cols:\n",
        "        return pd.DataFrame({\"msg\": [\"Sem colunas de sintomas (prefixo n√£o encontrado).\"]})\n",
        "\n",
        "    audit = pd.DataFrame({\n",
        "        \"non_nulls\": df[sintomas_cols].notna().sum(),\n",
        "        \"sum_positives\": df[sintomas_cols].sum(),\n",
        "        \"percent_positives\": df[sintomas_cols].mean() * 100\n",
        "    })\n",
        "    audit[\"status\"] = \"-\"\n",
        "    audit.loc[audit[\"sum_positives\"] == 0, \"status\"] = \"‚ö†Ô∏è Sempre 0\"\n",
        "    audit.loc[audit[\"non_nulls\"] == 0, \"status\"] = \"‚ö†Ô∏è Sempre NaN\"\n",
        "    return audit.sort_values(\"percent_positives\", ascending=False)\n",
        "\n",
        "\n",
        "def audit_feature_engineering(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Checagem r√°pida de presen√ßa e sanidade das principais novas features.\n",
        "    Retorna um pequeno quadro-resumo (counts/NA/descri√ß√£o de colunas-chave).\n",
        "    \"\"\"\n",
        "    cols_check = [\n",
        "        \"delay_notif\",\"tempo_internacao\",\"duracao_caso\",\n",
        "        \"ist_any\",\"ist_count\",\"imune_risco\",\"vacina_qualquer\",\n",
        "        \"faixa_etaria\",\"regiao\",\"sintoma_count\",\"imune_risco_x_sintomas\",\n",
        "        \"delay_notif_centered_regiao\",\n",
        "        \"target_hosp\",\"target_obito_any\",\"target_obito_mpx\"\n",
        "    ]\n",
        "    cols_check = [c for c in cols_check if c in df.columns]\n",
        "    if not cols_check:\n",
        "        return pd.DataFrame({\"msg\": [\"Nenhuma coluna derivada encontrada para auditar.\"]})\n",
        "\n",
        "    out = []\n",
        "    for c in cols_check:\n",
        "        s = df[c]\n",
        "        out.append({\n",
        "            \"col\": c,\n",
        "            \"dtype\": str(s.dtype),\n",
        "            \"non_null\": int(s.notna().sum()),\n",
        "            \"na\": int(s.isna().sum()),\n",
        "            \"unique\": int(s.nunique(dropna=True)),\n",
        "            \"sample_values\": s.dropna().unique()[:5] if s.nunique(dropna=True) <= 10 else \"many\"\n",
        "        })\n",
        "    return pd.DataFrame(out).set_index(\"col\")"
      ],
      "metadata": {
        "id": "WHE72Kwv-8I7"
      },
      "execution_count": null,
      "outputs": [],
      "id": "WHE72Kwv-8I7"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Estrutura de Execu√ß√£o (orquestra todas as etapas)**\n",
        "\n",
        "Neste momento como ainda est√°-se a explorar e atuar na Engenharia de Atributos, melhor manter todos os dummies de sintomas"
      ],
      "metadata": {
        "id": "JOLTxaRnmdNw"
      },
      "id": "JOLTxaRnmdNw"
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------------------------------------------------\n",
        "# 8) PIPELINE DE EXECU√á√ÉO (orquestra todas as etapas)\n",
        "# ------------------------------------------------------------\n",
        "def run_feature_engineering_pipeline(\n",
        "    df: pd.DataFrame,\n",
        "    sintomas_min_freq: int | None = None\n",
        ") -> tuple[pd.DataFrame, dict]:\n",
        "    \"\"\"\n",
        "    Executa a Engenharia de Atributos end-to-end:\n",
        "\n",
        "    Etapas:\n",
        "      1. Explos√£o de SINTOMA ‚Üí dummies (min_freq opcional; default preserva todos).\n",
        "      2. Deltas de datas (sem imputar; respeita NaT).\n",
        "      3. Agregados cl√≠nicos (ISTs, imuno, vacina).\n",
        "      4. Bins demogr√°ficos (idade, regi√£o).\n",
        "      5. Intera√ß√µes (ex.: imuno √ó sintomas, centraliza√ß√£o de delay por regi√£o).\n",
        "      6. Targets (hosp, √≥bito).\n",
        "      7. Auditorias.\n",
        "\n",
        "    Par√¢metros\n",
        "    ----------\n",
        "    df : DataFrame\n",
        "        DataFrame p√≥s-imputa√ß√£o/limpeza.\n",
        "    sintomas_min_freq : int | None\n",
        "        Cutoff para filtrar sintomas raros (None = mant√©m todos).\n",
        "\n",
        "    Retorno\n",
        "    -------\n",
        "    df_feat : DataFrame\n",
        "        DataFrame final enriquecido com todas as features derivadas.\n",
        "    info : dict\n",
        "        Dicion√°rio com artefatos √∫teis:\n",
        "          - 'sintomas_cols': lista de colunas dummies geradas\n",
        "          - 'audit_sintomas': DataFrame de auditoria de sintomas\n",
        "          - 'audit_features': DataFrame de auditoria final de features\n",
        "    \"\"\"\n",
        "    # 1) Sintomas ‚Üí dummies\n",
        "    df1, sx_cols = explode_sintoma_dummies_simple(df, col=\"SINTOMA\", prefix=\"sx_\", min_freq=sintomas_min_freq)\n",
        "\n",
        "    # 2) Datas ‚Üí deltas\n",
        "    df2 = add_date_features(df1)\n",
        "\n",
        "    # 3) Agregados cl√≠nicos\n",
        "    df3 = add_clinical_aggregates(df2)\n",
        "\n",
        "    # 4) Bins demogr√°ficos / regi√£o\n",
        "    df4 = add_demo_bins(df3)\n",
        "\n",
        "    # 5) Intera√ß√µes\n",
        "    df5 = add_interactions(df4)\n",
        "\n",
        "    # 6) Targets\n",
        "    df_feat = add_targets(df5)\n",
        "\n",
        "    # 7) Auditorias\n",
        "    audit_sx = audit_sintomas(df_feat, sx_cols)\n",
        "    audit_ft = audit_feature_engineering(df_feat)\n",
        "\n",
        "    info = {\n",
        "        \"sintomas_cols\": sx_cols,\n",
        "        \"audit_sintomas\": audit_sx,\n",
        "        \"audit_features\": audit_ft\n",
        "    }\n",
        "    return df_feat, info\n",
        "\n",
        "\n",
        "# ==========================================\n",
        "# üöÄ EXECU√á√ÉO REAL NO DATAFRAME\n",
        "# (assume que j√° existe mpox_2022_prep como df p√≥s-imputa√ß√£o)\n",
        "# ==========================================\n",
        "df0 = df_prep.copy()\n",
        "\n",
        "# ‚ñ∫ Mant√©m todos os sintomas (literatura/consenso: preservar informa√ß√£o cl√≠nica agora)\n",
        "df_feateng, fe_info = run_feature_engineering_pipeline(df0, sintomas_min_freq=None)\n",
        "\n",
        "print(\"\\n‚úîÔ∏è Engenharia de atributos conclu√≠da:\")\n",
        "print(\"\\nTotal de colunas:\", df_feateng.shape[1])\n",
        "print(\"Total de linhas :\", df_feateng.shape[0])\n"
      ],
      "metadata": {
        "id": "p4wLF8FqtFKI"
      },
      "execution_count": null,
      "outputs": [],
      "id": "p4wLF8FqtFKI"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **‚öôÔ∏è Auditoria P√≥s-Engenharia de Dados**"
      ],
      "metadata": {
        "id": "B19PnoRGe0AW"
      },
      "id": "B19PnoRGe0AW"
    },
    {
      "cell_type": "code",
      "source": [
        "# Amostra de auditorias\n",
        "print(\"\\n‚úîÔ∏è Resultado da Auditoria P√≥s-Engenharia de Dados:\")\n",
        "print(\"\\n[Audit] Dummies de sintomas (top 10 por % positivos):\")\n",
        "print(fe_info[\"audit_sintomas\"].head(10))\n",
        "\n",
        "print(\"\\n\\n[Audit] Quadro-resumo de features derivadas:\")\n",
        "print(fe_info[\"audit_features\"],'\\n')"
      ],
      "metadata": {
        "id": "OI7tH48ze0pm"
      },
      "execution_count": null,
      "outputs": [],
      "id": "OI7tH48ze0pm"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üìä Distribui√ß√£o das vari√°veis-alvo bin√°rias"
      ],
      "metadata": {
        "id": "Al3JHwqoNa2w"
      },
      "id": "Al3JHwqoNa2w"
    },
    {
      "cell_type": "code",
      "source": [
        "# üìä Distribui√ß√£o das vari√°veis-alvo bin√°rias (df_feateng) ‚Äì barras horizontais\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "target_cols = [\"target_hosp\", \"target_obito_any\", \"target_obito_mpx\"]\n",
        "\n",
        "fig, axes = plt.subplots(len(target_cols), 1, figsize=(12, 6))  # uma coluna, v√°rias linhas\n",
        "\n",
        "for i, target_col in enumerate(target_cols):\n",
        "    y = df_feateng[target_col].astype(int)\n",
        "\n",
        "    counts = y.value_counts().sort_index()\n",
        "    ratios = (counts / counts.sum() * 100).round(2)\n",
        "\n",
        "    bars = axes[i].barh([str(k) for k in counts.index], counts.values,\n",
        "                        color=[\"lightgreen\", \"green\"], alpha=1)\n",
        "\n",
        "    # Adicionar percentuais ao lado de cada barra\n",
        "    for bar, pct in zip(bars, ratios):\n",
        "        width = bar.get_width()\n",
        "        axes[i].text(width + (0.01*width), bar.get_y() + bar.get_height()/2,\n",
        "                     f\"{pct:.2f}%\", ha=\"left\", va=\"center\", fontsize=10)\n",
        "\n",
        "    axes[i].set_title(f\"{target_col}\")\n",
        "    axes[i].set_xlabel(\"Contagem\")\n",
        "    axes[i].set_ylabel(\"Classe\")\n",
        "\n",
        "plt.suptitle(\"üìä Distribui√ß√£o de classes ‚Äì Vari√°veis-alvo bin√°rias\", fontsize=14)\n",
        "plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "9ZXxm3y2NaBt"
      },
      "execution_count": null,
      "outputs": [],
      "id": "9ZXxm3y2NaBt"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üìä Verifica√ß√£o de Distribui√ß√£o dos Targets ap√≥s Engenharia de Atributos"
      ],
      "metadata": {
        "id": "TIqTEv3v9-xu"
      },
      "id": "TIqTEv3v9-xu"
    },
    {
      "cell_type": "code",
      "source": [
        "# üìä Distribui√ß√£o percentual dos targets + gr√°ficos lado a lado (Seaborn)\n",
        "#import seaborn as sns\n",
        "#import matplotlib.pyplot as plt\n",
        "\n",
        "# Lista de poss√≠veis targets\n",
        "targets = [\"target_hosp\", \"target_obito_any\", \"target_obito_mpx\"]\n",
        "\n",
        "# Impress√£o no console (percentuais simplificados)\n",
        "print(f\"Distribui√ß√£o (%) de:\\n\")\n",
        "for t in targets:\n",
        "    if t in df_feateng.columns:\n",
        "        counts = (df_feateng[t].value_counts(normalize=True) * 100).round(2)\n",
        "        distrib_dict = counts.to_dict()\n",
        "\n",
        "        print(f\"{t}: {distrib_dict}\")\n",
        "\n",
        "# Preparar dados em formato longo para o seaborn\n",
        "plot_data = []\n",
        "for t in targets:\n",
        "    if t in df_feateng.columns:\n",
        "        counts = (df_feateng[t].value_counts(normalize=True) * 100).round(2)\n",
        "        for cls, val in counts.items():\n",
        "            plot_data.append({\"Target\": t, \"Classe\": str(cls), \"Percentual\": val})\n",
        "\n",
        "df_plot = pd.DataFrame(plot_data)\n",
        "\n",
        "# Plot com seaborn ‚Äî um subplot por target\n",
        "n = df_plot[\"Target\"].nunique()\n",
        "fig, axes = plt.subplots(1, n, figsize=(5*n, 4), sharey=True)\n",
        "\n",
        "if n == 1:  # garante que axes seja iter√°vel mesmo com 1 target\n",
        "    axes = [axes]\n",
        "\n",
        "for ax, target in zip(axes, df_plot[\"Target\"].unique()):\n",
        "    sns.barplot(\n",
        "        data=df_plot[df_plot[\"Target\"] == target],\n",
        "        x=\"Classe\", y=\"Percentual\",\n",
        "        color=\"lightgreen\", edgecolor=\"gray\", linewidth=1.2, ax=ax\n",
        "    )\n",
        "    ax.set_title(f\"{target}\", fontsize=10)\n",
        "    ax.set_ylabel(\"Percentual (%)\")\n",
        "    ax.set_xlabel(\"Classe\")\n",
        "\n",
        "plt.suptitle(\"Distribui√ß√£o percentual dos targets\", fontsize=12, y=1.05)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "p1AWzS8lGlbm"
      },
      "execution_count": null,
      "outputs": [],
      "id": "p1AWzS8lGlbm"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üíæ Salvamento Final da Base P√≥s-Engenharia de Atributos"
      ],
      "metadata": {
        "id": "AVpOksWP91k9"
      },
      "id": "AVpOksWP91k9"
    },
    {
      "cell_type": "code",
      "source": [
        "# üíæ Salvamento final da base p√≥s-engenharia de atributos\n",
        "output_path = \"mpox_2022_feateng.csv\"\n",
        "df_feateng.to_csv(output_path, index=False, encoding=\"utf-8\")\n",
        "print(f\"‚úîÔ∏è Arquivo salvo em: {output_path}\")\n",
        "\n",
        "# üîç Verifica√ß√£o p√≥s-salvamento (recarrega e confere shape + distribui√ß√£o dos targets)\n",
        "df_check = pd.read_csv(output_path)\n",
        "print(\"\\nCarregado de volta com sucesso:\", df_check.shape)\n",
        "\n",
        "# Lista de poss√≠veis targets\n",
        "targets = [\"target_hosp\", \"target_obito_any\", \"target_obito_mpx\"]\n",
        "\n",
        "for t in targets:\n",
        "    if t in df_check.columns:\n",
        "        counts = (df_check[t].value_counts(normalize=True) * 100).round(2)\n",
        "        distrib_dict = counts.to_dict()\n",
        "        print(f\"Distribui√ß√£o (%) de {t}: {distrib_dict}\")\n"
      ],
      "metadata": {
        "id": "6xRH43kZtSSW"
      },
      "execution_count": null,
      "outputs": [],
      "id": "6xRH43kZtSSW"
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "##üßπ **Sele√ß√£o de Atributos ‚Äî Resumo das A√ß√µes**\n",
        "\n",
        "---\n",
        "\n",
        "- ‚úÇÔ∏è Elimina:\n",
        "\n",
        "  - Vari√°veis de datas (`DT_*`),  \n",
        "  - Identificadores t√©cnicos (`__arquivo_origem__`),  \n",
        "  - Texto livre cru (`OUTRO_DES`).  \n",
        "\n",
        "- üìâ Aplica filtro de vari√¢ncia:\n",
        "  - Remove vari√°veis num√©ricas com vari√¢ncia < 1% (sem poder discriminativo:\n",
        "              `VarianceThreshold(threshold=0.01)`).  \n",
        "\n",
        "- üéØ Mant√©m:\n",
        "  - Vari√°veis categ√≥ricas para uso em **codificadores** (LabelEncoder / OneHotEncoder)."
      ],
      "metadata": {
        "id": "XUAAz4hSoPQ8"
      },
      "id": "XUAAz4hSoPQ8"
    },
    {
      "cell_type": "markdown",
      "source": [
        "üßë Esclarecimento\n",
        "\n",
        "Filtro de vari√¢ncia:\n",
        "- A vari√¢ncia de uma feature √© uma medida estat√≠stica que indica o quanto os valores dessa vari√°vel se dispersam em rela√ß√£o √† m√©dia.\n",
        "\n",
        "- Atributos com baixa vari√¢ncia tendem a ser pouco informativos, pois n√£o ajudam a distinguir entre diferentes classes ou padr√µes nos dados.\n",
        "\n",
        "- Objetivo do filtro √© melhorar a qualidade dos dados antes de aplicar algoritmos de aprendizado.\n",
        "\n",
        "    ‚ÄúFeatures that do not vary much across the dataset are unlikely to be useful for prediction and can be removed during preprocessing.‚Äù ‚Äî Kelleher et al., 2nd Ed."
      ],
      "metadata": {
        "id": "tTYIGpJisSe5"
      },
      "id": "tTYIGpJisSe5"
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Sele√ß√£o de Atributos ‚Äî Remo√ß√£o inicial de invi√°veis + filtro de vari√¢ncia\n",
        "\n",
        "#import pandas as pd\n",
        "#from sklearn.feature_selection import VarianceThreshold\n",
        "\n",
        "# Carrega o dataset p√≥s-engenharia de atributos\n",
        "df_feateng = pd.read_csv(\"mpox_2022_feateng.csv\")\n",
        "\n",
        "print(f\"[0] Dataset carregado com {df_feateng.shape[1]} vari√°veis\")\n",
        "\n",
        "print(\"\\nTotal de vari√°veis ap√≥s:\\n\")\n",
        "# --------------------------\n",
        "# A) Remover vari√°veis invi√°veis\n",
        "# --------------------------\n",
        "inviaveis = [\n",
        "    \"__arquivo_origem__\",   # identificador t√©cnico\n",
        "    \"OUTRO_DES\"             # texto livre cru (j√° n√£o entra em modelagem)\n",
        "] + [c for c in df_feateng.columns if c.startswith(\"DT_\")]  # datas puras\n",
        "\n",
        "df_reduced = df_feateng.drop(columns=[c for c in inviaveis if c in df_feateng.columns])\n",
        "\n",
        "print(f\"[1] Remo√ß√£o de invi√°veis: {df_reduced.shape[1]} vari√°veis\")\n",
        "\n",
        "# --------------------------\n",
        "# B) Filtro de vari√¢ncia\n",
        "# --------------------------\n",
        "sel = VarianceThreshold(threshold=0.01)  # <1% de vari√¢ncia ‚Üí descartar\n",
        "sel.fit(df_reduced.select_dtypes(include=[\"int64\",\"float64\"]))\n",
        "\n",
        "variaveis_ok = df_reduced.select_dtypes(include=[\"int64\",\"float64\"]).columns[sel.get_support()]\n",
        "df_atribsel = df_reduced[variaveis_ok.tolist() + df_reduced.select_dtypes(include=[\"object\"]).columns.tolist()]\n",
        "\n",
        "print(f\"\\n[2] Filtro de vari√¢ncia: {df_atribsel.shape[1]} vari√°veis\")"
      ],
      "metadata": {
        "id": "lCPoYJ-BoTxO"
      },
      "execution_count": null,
      "outputs": [],
      "id": "lCPoYJ-BoTxO"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Filtro sem√¢ntico\n"
      ],
      "metadata": {
        "id": "zSIhboIf_Ojz"
      },
      "id": "zSIhboIf_Ojz"
    },
    {
      "cell_type": "code",
      "source": [
        "# --------------------------\n",
        "# C) üîé Filtro sem√¢ntico/manual ‚Äî remo√ß√£o de vari√°veis redundantes (ap√≥s filtro estat√≠stico)\n",
        "# --------------------------\n",
        "#\n",
        "redundant_cols = [\n",
        "    \"SG_UF\", \"CO_UF_RES\"  # exemplo redund√¢ncia sem√¢ntica com UF\n",
        "    # adiciona outras colunas redundantes aqui\n",
        "]\n",
        "\n",
        "df_atribsel = df_atribsel.drop(columns=[c for c in redundant_cols if c in df_atribsel.columns])\n",
        "\n",
        "print(f\"‚úÖ Vari√°veis redundantes removidas. \\n\\nTotal atual de vari√°veis no dataframe: {df_atribsel.shape[1]}\")\n"
      ],
      "metadata": {
        "id": "-2wprku1_Nny"
      },
      "execution_count": null,
      "outputs": [],
      "id": "-2wprku1_Nny"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üìä Diagn√≥stico de correla√ß√£o (apenas num√©ricas reais cont√≠nuas/discretas)"
      ],
      "metadata": {
        "id": "p_YGe8KaScAk"
      },
      "id": "p_YGe8KaScAk"
    },
    {
      "cell_type": "code",
      "source": [
        "# üìä Diagn√≥stico de correla√ß√£o (apenas num√©ricas reais cont√≠nuas/discretas)\n",
        "def corr_diag_numeric_only(df, threshold=0.9):\n",
        "    # Apenas avalia correla√ß√£o em colunas num√©ricas cont√≠nuas/discretas.\n",
        "    # N√£o remove nada ‚Äî s√≥ retorna pares correlacionados.\n",
        "    num_df = df.select_dtypes(include=[\"number\"])\n",
        "    corr_matrix = num_df.corr().abs()\n",
        "\n",
        "    high_corr = [\n",
        "        (i, j, corr_matrix.loc[i, j])\n",
        "        for i in corr_matrix.columns\n",
        "        for j in corr_matrix.columns\n",
        "        if i != j and corr_matrix.loc[i, j] > threshold\n",
        "    ]\n",
        "\n",
        "    return sorted(high_corr, key=lambda x: -x[2])\n",
        "\n",
        "\n",
        "\n",
        "# ‚ñ∂Ô∏è Execu√ß√£o: Diagn√≥stico de alta correla√ß√£o em vari√°veis num√©ricas (pr√©-encoding)\n",
        "\n",
        "high_corr_pairs = corr_diag_numeric_only(df_atribsel, threshold=0.9)\n",
        "\n",
        "if not high_corr_pairs:\n",
        "    print(\"\\n‚úÖ Nenhuma correla√ß√£o forte encontrada entre vari√°veis num√©ricas.\")\n",
        "else:\n",
        "    print(f\"\\n‚ö†Ô∏è Total de {len(high_corr_pairs)} pares de vari√°veis altamente correlacionadas encontrados.\")\n",
        "    print(\"\\n[Diagn√≥stico] Pares de vari√°veis altamente correlacionadas (>0.9):\")\n",
        "    for i, j, corr in high_corr_pairs:\n",
        "      print(f\" - {i} vs {j}: {corr:.2f}\")\n",
        "print('\\n')"
      ],
      "metadata": {
        "id": "xmCU4MwfSZLI"
      },
      "execution_count": null,
      "outputs": [],
      "id": "xmCU4MwfSZLI"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Salva dados P√≥s-Sele√ß√£o de Atributos"
      ],
      "metadata": {
        "id": "iLAyNAE6AoTd"
      },
      "id": "iLAyNAE6AoTd"
    },
    {
      "cell_type": "code",
      "source": [
        "# --------------------------\n",
        "# Exporta vers√£o reduzida\n",
        "# --------------------------\n",
        "out_path = \"mpox_2022_atribsel.csv\"\n",
        "df_atribsel.to_csv(out_path, index=False)\n",
        "print(f\"\\n[3] Dataset salvo como: {out_path}\\n\")"
      ],
      "metadata": {
        "id": "amrYgXJU_jEh"
      },
      "execution_count": null,
      "outputs": [],
      "id": "amrYgXJU_jEh"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üìå Nota da se√ß√£o ‚Äì Dataset ap√≥s Engenharia e Sele√ß√£o de Atributos\n",
        "\n",
        "- `df_feateng` ‚Üí sa√≠da da Engenharia de Atributos; √© um dataframe derivado de df_prep\n",
        "- `df_atribsel` ‚Üí dataframe derivado de `df_feateng`, ap√≥s sele√ß√£o inicial (remo√ß√£o de invi√°veis + filtro de vari√¢ncia + filtro sem√¢ntico/manual):  \n",
        "  - Objetivo: evitar redund√¢ncias e reduzir multicolinearidade.  \n",
        "  - Este dataframe ser√° utilizado como entrada na etapa de **Codifica√ß√£o (Encoding)**.\n"
      ],
      "metadata": {
        "id": "bBhCadf3fkBg"
      },
      "id": "bBhCadf3fkBg"
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## **Atividade 3: Codifica√ß√£o de vari√°veis categ√≥ricas (OrdinalEncoder e OneHotEncoder)**\n",
        "---\n",
        "\n",
        "-  Como n√£o se tem vari√°veis categ√≥ricas ordinais, somente OneHotEncoder ser√° **utilizado**"
      ],
      "metadata": {
        "id": "z8ju2I-CKsd_"
      },
      "id": "z8ju2I-CKsd_"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Vetores de Colunas por Tipo de Vari√°veis"
      ],
      "metadata": {
        "id": "EYlFQJV6ozrk"
      },
      "id": "EYlFQJV6ozrk"
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================\n",
        "# CATEG√ìRICAS NOMINAIS (texto / c√≥digos IBGE)\n",
        "# ================================\n",
        "nominal_categorical_cols = [\n",
        "    'SG_UF',        # string, sigla da UF\n",
        "    'SG_UF_NOT',    # string, sigla da UF da notifica√ß√£o\n",
        "    'ID_MUNICIP',   # int64 (c√≥digo IBGE)\n",
        "    'CO_UF_RES',    # int64 (c√≥digo IBGE da UF resid√™ncia)\n",
        "    'ID_MN_RESI'    # int64 (c√≥digo IBGE munic√≠pio resid√™ncia)\n",
        "]\n",
        "\n",
        "# ================================\n",
        "# CATEG√ìRICAS M√öLTIPLAS (lista em texto)\n",
        "# ================================\n",
        "list_multiple_categorical_cols = [\n",
        "    'SINTOMA'   # object (lista de sintomas separados por v√≠rgula)\n",
        "]\n",
        "\n",
        "# ================================\n",
        "# TEXTO LIVRE\n",
        "# ================================\n",
        "text_cols = [\n",
        "    'OUTRO_DES'   # object\n",
        "]\n",
        "\n",
        "# ================================\n",
        "# NUM√âRICAS CATEG√ìRICAS BIN√ÅRIAS (0/1 ap√≥s recodifica√ß√£o)\n",
        "# ================================\n",
        "numerical_as_binary_categ_cols = [\n",
        "    'GONORREIA','CLAMIDEA','SIFILIS','HERPES_GENITAL','CANCRO_MOLE',\n",
        "    'TRICHOMOMAS_VAGINALS','LINFOGRANULOMA','MYCOPLASMA_GENITAL',\n",
        "    'HPV','DIP','DONOVANOSE','HTLV','VERRUGA_GENITAL'\n",
        "]\n",
        "\n",
        "# ================================\n",
        "# NUM√âRICAS CATEG√ìRICAS NOMINAIS (c√≥digos)\n",
        "# ================================\n",
        "numerical_as_nominal_categ_cols = [\n",
        "    'CLASSI_FIN', 'CS_SEXO','COMP_SEXUAL','ORIENTA_SEXUAL','IDENT_GENERO',\n",
        "    'CS_RACA','PAC_IMUNOSSUP','IST_ATIVA','TP_AMOST','ESTRANGEIRO',\n",
        "    'HIV','UTI','RESULTADO_EXA_LAB','PROFIS_SAUDE','VACINA',\n",
        "    'TRATAMENTO_MONKEYPOX','HOSPITAL','EVOLUCAO','VINCULO_EPI',\n",
        "    'LOCAL_CONT','TRANSM','CONTAT_ANIMAL', 'CS_GESTANT',\n",
        "    'MET_LAB','CARACT_GENOMICA','CLADO',\n",
        "    'DOENCA_TRA.1'   # ‚ö†Ô∏è ajustado: n√£o √© texto, √© c√≥digo categ√≥rico\n",
        "]\n",
        "\n",
        "# ==============COMENTADO==================\n",
        "# CATEG√ìRICAS ORDINAIS (num√©ricas com hierarquia natural)\n",
        "# ================================\n",
        "#numerical_as_ordinal_categ_cols = [\n",
        "#    'CS_GESTANT'  # int64 ‚Üí ordinal (ordem gestacional)]\n",
        "\n",
        "# =============COMENTADO===================\n",
        "# NUM√âRICAS REAIS / CONT√çNUAS\n",
        "# ================================\n",
        "\n",
        "# ================================\n",
        "# NUM√âRICAS REAIS / DISCRETAS\n",
        "# ================================\n",
        "numerical_discrete_cols = [\n",
        "    'NU_IDADE_N',   # int64 ‚Üí idade em anos (discreta)\n",
        "    'CONTAG_CD4'    # contagem de c√©lulas CD4 (discreta, mas pode ser analisada como cont√≠nua)\n",
        "]\n",
        "\n",
        "# ================================\n",
        "# TEMPORAIS (datas)\n",
        "# ================================\n",
        "date_temporal_cols = [\n",
        "    'DT_NOTIFIC','DT_SIN_PRI','DT_COLETA','DATA_VACINA',\n",
        "    'DT_INTERNA','DT_EVOLUCAO'   # todos devem virar datetime64[ns]\n",
        "]\n",
        "\n",
        "# ================================\n",
        "# IDENTIFICADOR T√âCNICO\n",
        "# ================================\n",
        "identif_cols = [\n",
        "    '__arquivo_origem__'  # object, s√≥ para rastreabilidade\n",
        "]\n",
        "\n"
      ],
      "metadata": {
        "id": "E0bmTkQ2LcYX"
      },
      "execution_count": null,
      "outputs": [],
      "id": "E0bmTkQ2LcYX"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Codifica√ß√£o OneHotEncoder**"
      ],
      "metadata": {
        "id": "BpDDTxi3Kt8d"
      },
      "id": "BpDDTxi3Kt8d"
    },
    {
      "cell_type": "code",
      "source": [
        "# 1) Codifica√ß√£o de vari√°veis categ√≥ricas (One-Hot)\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "# Usa vetores j√° definidos (garantem todas categ√≥ricas relevantes)\n",
        "onehot_cols = [c for c in df_atribsel.columns\n",
        "               if c in nominal_categorical_cols\n",
        "               or c in numerical_as_nominal_categ_cols]\n",
        "\n",
        "# Inicializa encoder\n",
        "encoder = OneHotEncoder(drop='first', sparse_output=False, dtype=int, handle_unknown='ignore')\n",
        "\n",
        "# Aplica transform\n",
        "encoded = encoder.fit_transform(df_atribsel[onehot_cols])\n",
        "\n",
        "# Converte para DataFrame\n",
        "encoded_df = pd.DataFrame(encoded,\n",
        "                          columns=encoder.get_feature_names_out(onehot_cols),\n",
        "                          index=df_atribsel.index)\n",
        "\n",
        "# Junta e remove originais\n",
        "df_encoded = pd.concat([df_atribsel.drop(columns=onehot_cols), encoded_df], axis=1)\n",
        "\n",
        "print(\"[Encoding] Dataset final:\", df_encoded.shape)\n",
        "print(\"[Encoding] Categ√≥ricas originais:\", len(onehot_cols))\n",
        "print(\"[Encoding] Vari√°veis ap√≥s OneHot:\", encoded_df.shape[1])\n"
      ],
      "metadata": {
        "id": "h9lgdAQPKxGF"
      },
      "execution_count": null,
      "outputs": [],
      "id": "h9lgdAQPKxGF"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Auditoria P√≥s-codifica√ß√£o de Vari√°veis Categ√≥ricas"
      ],
      "metadata": {
        "id": "J14g1BtacZzF"
      },
      "id": "J14g1BtacZzF"
    },
    {
      "cell_type": "code",
      "source": [
        "# 2) Auditoria p√≥s-codifica√ß√£o\n",
        "print(\"=== Auditoria P√≥s-codifica√ß√£o de Vari√°veis Categ√≥ricas ===\")\n",
        "\n",
        "# Total de colunas antes e depois\n",
        "print(f\"- Total antes da codifica√ß√£o: {df_atribsel.shape[1]}\")\n",
        "print(f\"- Total depois da codifica√ß√£o: {df_encoded.shape[1]}\")\n",
        "\n",
        "# Conferir se n√£o h√° mais colunas categ√≥ricas \"cruas\"\n",
        "restantes_cat = df_encoded.select_dtypes(include=['object', 'category']).columns\n",
        "if len(restantes_cat) > 0:\n",
        "    print(\"‚ö†Ô∏è Restaram colunas categ√≥ricas sem codifica√ß√£o:\", list(restantes_cat))\n",
        "else:\n",
        "    print(\"‚úÖ Todas as categ√≥ricas foram codificadas.\")\n",
        "\n",
        "# Conferir tipos predominantes\n",
        "print(\"\\nDtypes principais no dataset final:\")\n",
        "print(df_encoded.dtypes.value_counts())\n",
        "\n",
        "# Conferir presen√ßa de NaNs\n",
        "nulos = df_encoded.isna().sum().sum()\n",
        "print(f\"\\nValores ausentes ap√≥s codifica√ß√£o: {nulos}\")\n"
      ],
      "metadata": {
        "id": "YPf0aVbYcYRZ"
      },
      "execution_count": null,
      "outputs": [],
      "id": "YPf0aVbYcYRZ"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ‚ñ∂Ô∏è **Filtro de Alta Correla√ß√£o na P√≥s-codifica√ß√£o**"
      ],
      "metadata": {
        "id": "lAw5zRgGOKdD"
      },
      "id": "lAw5zRgGOKdD"
    },
    {
      "cell_type": "code",
      "source": [
        "# 3) Filtro de Alta Correla√ß√£o na P√≥s-codifica√ß√£o\n",
        "import numpy as np\n",
        "\n",
        "def drop_high_corr(df, threshold=0.9):\n",
        "    \"\"\"\n",
        "    Remove vari√°veis altamente correlacionadas.\n",
        "\n",
        "    Par√¢metros:\n",
        "    - df : DataFrame (num√©rico, j√° codificado)\n",
        "    - threshold : float (correla√ß√£o acima desse valor ser√° considerada redundante)\n",
        "\n",
        "    Retorna:\n",
        "    - df_reduced : DataFrame sem colunas redundantes\n",
        "    - dropped : lista de colunas removidas\n",
        "    \"\"\"\n",
        "    # Seleciona apenas num√©ricas (evita erro com datas)\n",
        "    num_df = df.select_dtypes(include=[np.number])\n",
        "\n",
        "    # Calcula matriz de correla√ß√£o\n",
        "    corr = num_df.corr().abs()\n",
        "\n",
        "    # Seleciona parte triangular superior\n",
        "    upper = corr.where(np.triu(np.ones(corr.shape), k=1).astype(bool))\n",
        "\n",
        "    # Colunas a remover\n",
        "    to_drop = [col for col in upper.columns if any(upper[col] > threshold)]\n",
        "\n",
        "    df_reduced = df.drop(columns=to_drop, errors=\"ignore\")\n",
        "    return df_reduced, to_drop\n",
        "\n",
        "\n",
        "# ‚ñ∂Ô∏è Aplica√ß√£o: no df_encoded (j√° p√≥s-codifica√ß√£o)\n",
        "df_atribsel, dropped_corr = drop_high_corr(df_encoded, threshold=0.9)\n",
        "\n",
        "print(f\"[Filtro de Alta Correla√ß√£o] Colunas removidas: {len(dropped_corr)}\")\n",
        "#debug# print(\"Exemplo de colunas removidas:\", dropped_corr[:10])\n",
        "print(\"Shape final:\", df_atribsel.shape)\n"
      ],
      "metadata": {
        "id": "DQwA9zMWOLM7"
      },
      "execution_count": null,
      "outputs": [],
      "id": "DQwA9zMWOLM7"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Auditoria P√≥s-filtro de Alta Correla√ß√£o"
      ],
      "metadata": {
        "id": "FQR-BXlqlkP1"
      },
      "id": "FQR-BXlqlkP1"
    },
    {
      "cell_type": "code",
      "source": [
        "# Auditoria P√≥s-filtro de Correla√ß√£o na P√≥s-codifica√ß√£o\n",
        "\n",
        "print(\"=== Auditoria P√≥s-Correla√ß√£o ===\")\n",
        "print(f\"- Total de colunas antes: {df_encoded.shape[1]}\")\n",
        "print(f\"- Total de colunas depois: {df_atribsel.shape[1]}\")\n",
        "print(f\"- Colunas removidas por alta correla√ß√£o: {len(dropped_corr)}\")\n",
        "\n",
        "if dropped_corr:\n",
        "    print(\"\\n‚ö†Ô∏è Lista das primeiras colunas removidas:\\n\", dropped_corr[:15])\n",
        "else:\n",
        "    print(\"\\n‚úÖ Nenhuma coluna altamente correlacionada foi removida.\")\n",
        "\n",
        "# Conferir se restaram colunas categ√≥ricas (n√£o deveria)\n",
        "restantes_cat = df_atribsel.select_dtypes(include=['object', 'category']).columns\n",
        "if len(restantes_cat) > 0:\n",
        "    print(\"\\n‚ö†Ô∏è Restaram colunas categ√≥ricas:\\n\", list(restantes_cat))\n",
        "else:\n",
        "    print(\"\\n‚úÖ Apenas num√©ricas, pronto para escalonamento.\")\n",
        "\n",
        "print('\\n')"
      ],
      "metadata": {
        "id": "QIu9OVB-ljWd"
      },
      "execution_count": null,
      "outputs": [],
      "id": "QIu9OVB-ljWd"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üîé Notas sobre trade-offs da codifica√ß√£o de vari√°veis categ√≥ricas\n",
        "\n",
        "- **OrdinalEncoder**: preserva ordem entre categorias, mas introduz risco de assumir rela√ß√µes num√©ricas inexistentes (ex.: \"azul\" < \"verde\").\n",
        "- **OneHotEncoder**: evita ordens artificiais, mas pode gerar explos√£o de dimensionalidade em vari√°veis com muitas categorias.\n",
        "- **Trade-off pr√°tico neste projeto**:\n",
        "  - Vari√°veis com ordem natural ‚Üí `OrdinalEncoder`.\n",
        "  - Vari√°veis nominais sem ordem ‚Üí `OneHotEncoder`.\n",
        "- Essa escolha equilibra **interpretabilidade** (menos colunas) e **capacidade preditiva** (evitar falsas rela√ß√µes).\n",
        "\n",
        "üéó Lembrete: sem vari√°veis categ√≥ricas ordinais neste dataset, visto que `CS_GESTANTE`n√£o pode ser considerada como ordinal (√© somente em parte)\n"
      ],
      "metadata": {
        "id": "ci66h4-TwJrV"
      },
      "id": "ci66h4-TwJrV"
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## **Atividade 4: Escalonamento de vari√°veis num√©ricas (StandardScaler e MinMaxScaler)**\n",
        "\n",
        "##### Percep√ß√£o anal√≠tica: \"Idade tem muitos outliers em 80+, mas o StandardScaler reduz o impacto no treino do modelo, enquanto o MinMax preserva os extremos comprimidos em 1.\"\n",
        "---"
      ],
      "metadata": {
        "id": "HskBbrCBsEQB"
      },
      "id": "HskBbrCBsEQB"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Escalonamento StandardScaler**"
      ],
      "metadata": {
        "id": "dRd-BNIluPDR"
      },
      "id": "dRd-BNIluPDR"
    },
    {
      "cell_type": "code",
      "source": [
        "# 4.1) Escalonamento de vari√°veis num√©ricas ‚Äî StandardScaler\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "def scale_with_standard(df):\n",
        "    \"\"\"\n",
        "    Aplica StandardScaler (z-score) √†s vari√°veis num√©ricas.\n",
        "    Retorna:\n",
        "    - df_scaled_std : dataframe escalonado\n",
        "    - num_cols : lista de colunas num√©ricas escalonadas\n",
        "    \"\"\"\n",
        "    num_cols = df.select_dtypes(include=[np.number]).columns\n",
        "    scaler = StandardScaler()\n",
        "\n",
        "    df_scaled_std = df.copy()\n",
        "    df_scaled_std[num_cols] = scaler.fit_transform(df[num_cols])\n",
        "\n",
        "    return df_scaled_std, num_cols\n",
        "\n",
        "\n",
        "# ‚ñ∂Ô∏è Execu√ß√£o 4.1\n",
        "df_scaled_std, num_cols_std = scale_with_standard(df_atribsel)\n",
        "print(f\"[4.1] StandardScaler aplicado em {len(num_cols_std)} vari√°veis num√©ricas.\\n\")\n"
      ],
      "metadata": {
        "id": "8s4W2d7TsnEE"
      },
      "execution_count": null,
      "outputs": [],
      "id": "8s4W2d7TsnEE"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Auditoria P√≥s-Escalonamento ‚Äî StandardScaler"
      ],
      "metadata": {
        "id": "Hs8zw7KtW2TW"
      },
      "id": "Hs8zw7KtW2TW"
    },
    {
      "cell_type": "code",
      "source": [
        "# Auditoria 4.1 ‚Äî StandardScaler\n",
        "def audit_standard_scaling(df_original, df_scaled, cols):\n",
        "    \"\"\"\n",
        "    Verifica se StandardScaler funcionou:\n",
        "    - m√©dia ‚âà 0\n",
        "    - desvio padr√£o ‚âà 1\n",
        "    \"\"\"\n",
        "    audit = {}\n",
        "    for col in cols:\n",
        "        sc = df_scaled[col]\n",
        "        audit[col] = {\n",
        "            \"mean_scaled\": float(sc.mean()),\n",
        "            \"std_scaled\": float(sc.std())\n",
        "        }\n",
        "    return pd.DataFrame(audit).T\n",
        "\n",
        "audit_std = audit_standard_scaling(df_atribsel, df_scaled_std, num_cols_std)\n",
        "print(audit_std.head(), '\\n')\n"
      ],
      "metadata": {
        "id": "2mI3wC1cW7BA"
      },
      "execution_count": null,
      "outputs": [],
      "id": "2mI3wC1cW7BA"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Escalonamento de vari√°veis num√©ricas ‚Äî MinMaxScaler**"
      ],
      "metadata": {
        "id": "LhphFguKwIEq"
      },
      "id": "LhphFguKwIEq"
    },
    {
      "cell_type": "code",
      "source": [
        "# 4.2) Escalonamento de vari√°veis num√©ricas ‚Äî MinMaxScaler\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "def scale_with_minmax(df):\n",
        "    \"\"\"\n",
        "    Aplica MinMaxScaler [0,1] √†s vari√°veis num√©ricas.\n",
        "    Retorna:\n",
        "    - df_scaled_mm : dataframe escalonado\n",
        "    - num_cols : lista de colunas num√©ricas escalonadas\n",
        "    \"\"\"\n",
        "    num_cols = df.select_dtypes(include=[np.number]).columns\n",
        "    scaler = MinMaxScaler()\n",
        "\n",
        "    df_scaled_mm = df.copy()\n",
        "    df_scaled_mm[num_cols] = scaler.fit_transform(df[num_cols])\n",
        "\n",
        "    return df_scaled_mm, num_cols\n",
        "\n",
        "\n",
        "# ‚ñ∂Ô∏è Execu√ß√£o 4.2\n",
        "df_scaled_mm, num_cols_mm = scale_with_minmax(df_atribsel)\n",
        "print(f\"[4.2] MinMaxScaler aplicado em {len(num_cols_mm)} vari√°veis num√©ricas.\")\n"
      ],
      "metadata": {
        "id": "uXlaYzoPvHxc"
      },
      "execution_count": null,
      "outputs": [],
      "id": "uXlaYzoPvHxc"
    },
    {
      "cell_type": "markdown",
      "source": [
        " ### Auditoria P√≥s-Escalonamento ‚Äî MinMaxScaler"
      ],
      "metadata": {
        "id": "xegl83s5wU1-"
      },
      "id": "xegl83s5wU1-"
    },
    {
      "cell_type": "code",
      "source": [
        "# Auditoria 4.2 ‚Äî MinMaxScaler\n",
        "def audit_minmax_scaling(df_original, df_scaled, cols):\n",
        "    \"\"\"\n",
        "    Verifica se MinMaxScaler funcionou:\n",
        "    - min = 0\n",
        "    - max = 1\n",
        "    \"\"\"\n",
        "    audit = {}\n",
        "    for col in cols:\n",
        "        sc = df_scaled[col]\n",
        "        audit[col] = {\n",
        "            \"min_scaled\": float(sc.min()),\n",
        "            \"max_scaled\": float(sc.max())\n",
        "        }\n",
        "    return pd.DataFrame(audit).T\n",
        "\n",
        "audit_mm = audit_minmax_scaling(df_atribsel, df_scaled_mm, num_cols_mm)\n",
        "print(audit_mm.head())\n"
      ],
      "metadata": {
        "id": "Ay3HyalFvtqI"
      },
      "execution_count": null,
      "outputs": [],
      "id": "Ay3HyalFvtqI"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üìä Compara√ß√£o gr√°fica e estat√≠stica dos escalonadores"
      ],
      "metadata": {
        "id": "7H0pPG4CWWGm"
      },
      "id": "7H0pPG4CWWGm"
    },
    {
      "cell_type": "code",
      "source": [
        "# üìä Compara√ß√£o gr√°fica e estat√≠stica dos escalonadores\n",
        "\n",
        "# Usar diretamente as colunas num√©ricas discretas j√° definidas\n",
        "focus_vars = numerical_discrete_cols  # Ex.: [\"NU_IDADE_N\", \"CONTAG_CD4\"]\n",
        "\n",
        "FIGSIZE = (18, 8)  # Altura fixa, j√° que ser√° s√≥ uma linha de gr√°ficos\n",
        "figsPerLine = 3    # N√∫mero de figuras por linha\n",
        "\n",
        "colors = [\"lightgreen\", \"blue\", \"green\"]\n",
        "titles = [\"Original (sem escalonamento)\", \"Ap√≥s StandardScaler\", \"Ap√≥s MinMaxScaler\"]\n",
        "\n",
        "scaler_std = StandardScaler()\n",
        "scaler_mm = MinMaxScaler()\n",
        "\n",
        "# Aplicar transforma√ß√µes e manter nomes das colunas\n",
        "df_scaled_std = pd.DataFrame(\n",
        "    scaler_std.fit_transform(df_atribsel[focus_vars].astype(float)),\n",
        "    columns=focus_vars\n",
        ")\n",
        "df_scaled_mm = pd.DataFrame(\n",
        "    scaler_mm.fit_transform(df_atribsel[focus_vars].astype(float)),\n",
        "    columns=focus_vars\n",
        ")\n",
        "\n",
        "# ----------------------------------------------------------\n",
        "# Boxplots lado a lado (Antes, P√≥s-Std, P√≥s-MinMax)\n",
        "# ----------------------------------------------------------\n",
        "datasets = {\n",
        "    \"Original (sem escalonamento)\": df_atribsel[focus_vars].astype(float).values,\n",
        "    \"Ap√≥s StandardScaler\": df_scaled_std.values,\n",
        "    \"Ap√≥s MinMaxScaler\": df_scaled_mm.values\n",
        "}\n",
        "\n",
        "fig, axes = plt.subplots(1, figsPerLine, figsize=FIGSIZE)\n",
        "\n",
        "for ax, (title, data), color in zip(axes, datasets.items(), colors):\n",
        "    ax.boxplot(data, labels=focus_vars, vert=True, patch_artist=True,\n",
        "               boxprops=dict(facecolor=color))\n",
        "    ax.set_title(title, fontsize=12)\n",
        "    ax.tick_params(axis='x', rotation=45)\n",
        "\n",
        "plt.suptitle(\"üìä Compara√ß√£o dos Escalonadores ‚Äì Vari√°veis Num√©ricas Discretas\", fontsize=14)\n",
        "plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
        "plt.show()\n",
        "\n",
        "# ----------------------------------------------------------\n",
        "# Estat√≠sticas comparativas em formato matricial\n",
        "# ----------------------------------------------------------\n",
        "def resumo_stats_matriz(df_before, df_std, df_mm, cols):\n",
        "    stats = {}\n",
        "    for col in cols:\n",
        "        stats[col] = {\n",
        "            (\"Antes\", \"M√©dia\"):  df_before[col].mean(),\n",
        "            (\"Antes\", \"Desvio\"): df_before[col].std(),\n",
        "            (\"Antes\", \"Min\"):    df_before[col].min(),\n",
        "            (\"Antes\", \"Max\"):    df_before[col].max(),\n",
        "\n",
        "            (\"P√≥s-Std\", \"M√©dia\"):  df_std[col].mean(),\n",
        "            (\"P√≥s-Std\", \"Desvio\"): df_std[col].std(),\n",
        "            (\"P√≥s-Std\", \"Min\"):    df_std[col].min(),\n",
        "            (\"P√≥s-Std\", \"Max\"):    df_std[col].max(),\n",
        "\n",
        "            (\"P√≥s-MinMax\", \"M√©dia\"):  df_mm[col].mean(),\n",
        "            (\"P√≥s-MinMax\", \"Desvio\"): df_mm[col].std(),\n",
        "            (\"P√≥s-MinMax\", \"Min\"):    df_mm[col].min(),\n",
        "            (\"P√≥s-MinMax\", \"Max\"):    df_mm[col].max(),\n",
        "        }\n",
        "\n",
        "    df_stats = pd.DataFrame(stats).T.round(3)\n",
        "    df_stats.columns = pd.MultiIndex.from_tuples(df_stats.columns)  # cria hierarquia\n",
        "    return df_stats\n",
        "\n",
        "comparativo_stats_matriz = resumo_stats_matriz(\n",
        "    df_atribsel[focus_vars], df_scaled_std, df_scaled_mm, focus_vars\n",
        ")\n",
        "\n",
        "print(\"\\n=== Estat√≠sticas comparativas ===\")\n",
        "display(comparativo_stats_matriz)\n"
      ],
      "metadata": {
        "id": "aO-umamiWSED"
      },
      "execution_count": null,
      "outputs": [],
      "id": "aO-umamiWSED"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üìå Nota da se√ß√£o ‚Äì Escolha do escalonador para modelagem\n",
        "\n",
        "- As vari√°veis num√©ricas do dataset (`NU_IDADE_N`, `CONTAG_CD4`) s√£o **discretas** ‚Äì n√£o existem vari√°veis cont√≠nuas reais neste caso.\n",
        "\n",
        "- Foram avaliadas duas estrat√©gias de escalonamento: `StandardScaler` e `MinMaxScaler`.  \n",
        "- A compara√ß√£o gr√°fica e estat√≠stica mostrou que ambos funcionam conforme o esperado:\n",
        "  - `Original (sem escalonamento)`: mant√©m m√©dias, desvios, m√≠nimos e m√°ximos originais.\n",
        "  - `StandardScaler`: centraliza vari√°veis em m√©dia ‚âà 0 e desvio ‚âà 1.  \n",
        "    - _√ötil para modelos que assumem distribui√ß√£o aproximadamente gaussiana (ex.: Regress√£o Log√≠stica, Redes Neurais)._\n",
        "\n",
        "  - `MinMaxScaler`: normaliza vari√°veis para o intervalo [0, 1].  \n",
        "    - _Essencial para modelos baseados em dist√¢ncia (ex.: kNN, SVM)._\n",
        "\n",
        "\n",
        "- **Decis√£o metodol√≥gica:**  \n",
        "  - Ambos os escalonadores ser√£o mantidos no pipeline de pr√©-processamento.  \n",
        "  - A escolha final ser√° feita na **Parte 2 (Modelagem)**, comparando o impacto de cada abordagem no desempenho dos modelos sens√≠veis √† escala (ex.: kNN, SVM, Redes Neurais).\n",
        "  \n",
        "  ## üìå Nota da se√ß√£o ‚Äì Impacto do escalonamento nas vari√°veis num√©ricas discretas\n",
        "\n",
        "- A compara√ß√£o foi feita em **dois n√≠veis**:\n",
        "  1. **Visual (boxplots)** ‚Üí mostram a mudan√ßa da distribui√ß√£o antes e depois de aplicar os escalonadores.\n",
        "  2. **Num√©rica (tabela comparativa)** ‚Üí confirma estatisticamente os efeitos esperados.\n",
        "- **StandardScaler:**\n",
        "- **MinMaxScaler:**\n",
        "  - Normaliza cada vari√°vel para o intervalo **[0, 1]**.- **Decis√£o pr√°tica:**\n",
        "  - O uso de ambos os escalonadores ser√° considerado nos pipelines, avaliando impacto direto no desempenho dos modelos sens√≠veis √† escala.\n",
        "\n",
        "\n",
        "- As vari√°veis num√©ricas do dataset (`NU_IDADE_N`, `CONTAG_CD4`) s√£o **discretas** ‚Äì n√£o existem vari√°veis cont√≠nuas reais neste caso.\n"
      ],
      "metadata": {
        "id": "B4H5wijhaOMR"
      },
      "id": "B4H5wijhaOMR"
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "## **Divis√£o de preditoras (X) e alvo (y)**\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "mjRFb0vXayqQ"
      },
      "id": "mjRFb0vXayqQ"
    },
    {
      "cell_type": "code",
      "source": [
        "# 5.0 Divis√£o de preditoras (X) e alvo (y)\n",
        "# ----------------------------------------\n",
        "# - Objetivo: separar features (X) e vari√°vel-alvo (y).\n",
        "# - Aqui voc√™ escolhe QUAL alvo quer balancear (hospitaliza√ß√£o ou √≥bito).\n",
        "# - Pr√©-requisito: df_encoded (ou df_enc) j√° codificado (OneHot) e sem colunas n√£o-num√©ricas.\n",
        "# - Observa√ß√£o: por ora, vamos seguir com 'target_hosp'; depois voc√™ pode trocar pelo alvo de √≥bito.\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Escolhe o alvo aqui:\n",
        "target_col = \"target_hosp\"   # alternativas: \"target_obito_any\", \"target_obito_mpx\" (se estiverem com menor desbalanceamento)\n",
        "\n",
        "# Verifica√ß√µes b√°sicas\n",
        "assert target_col in df_encoded.columns, f\"Alvo '{target_col}' n√£o existe em df_encoded.\"\n",
        "assert set(df_encoded[target_col].dropna().unique()) <= {0,1}, \"Alvo precisa ser bin√°rio (0/1).\"\n",
        "\n",
        "# Separa X e y\n",
        "X = df_encoded.drop(columns=[target_col]).copy()\n",
        "y = df_encoded[target_col].copy()\n",
        "\n",
        "print(\"[5.0] X e y criados:\", X.shape, y.shape)\n",
        "print(\"Distribui√ß√£o (%) do alvo selecionado:\",\n",
        "      (y.value_counts(normalize=True)*100).round(2).to_dict(), '\\n')\n"
      ],
      "metadata": {
        "id": "Gg01OEkLavyk"
      },
      "execution_count": null,
      "outputs": [],
      "id": "Gg01OEkLavyk"
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## **Atividade 5: Balanceamento dos dados (Tomek e Smote)**\n",
        "- Checar se os dados est√£o desbalanceados\n",
        "- Aplicar os algoritmos Tomek e Smote\n",
        "- Avaliar impacto no desempenho dos modelos treinados\n",
        "---"
      ],
      "metadata": {
        "id": "ebBpIesASikf"
      },
      "id": "ebBpIesASikf"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Checagem de desbalanceamento das classes\n"
      ],
      "metadata": {
        "id": "oI-Fh2VVbyxf"
      },
      "id": "oI-Fh2VVbyxf"
    },
    {
      "cell_type": "code",
      "source": [
        "# 5.1 Checagem de desbalanceamento das classes\n",
        "# --------------------------------------------\n",
        "# - Objetivo: medir a propor√ß√£o das classes antes de balancear.\n",
        "# - Sa√≠da: distribui√ß√£o percentual e alerta visual simples.\n",
        "\n",
        "from collections import Counter\n",
        "\n",
        "dist_original = (y.value_counts(normalize=True) * 100).round(2).to_dict()\n",
        "print(f\"\\nVari√°vel-alvo: {target_col}\")\n",
        "print(\"\\n[5.1] Distribui√ß√£o (%) do alvo (Original -> P√≥s-codifica√ß√£o/escalonamento):\\n\", dist_original)\n",
        "\n",
        "# Regras pr√°ticas (heur√≠stica):\n",
        "#   - Se min(classe) < 20% ‚Üí dataset bastante desbalanceado\n",
        "#   - Entre 20% e 40% ‚Üí moderadamente desbalanceado\n",
        "minority_pct = min(dist_original.values())\n",
        "if minority_pct < 20:\n",
        "    print(\"\\n‚ö†Ô∏è Alerta: classe minorit√°ria < 20% (desbalanceamento alto).\")\n",
        "elif minority_pct < 40:\n",
        "    print(\"‚ÑπÔ∏è Observa√ß√£o: desbalanceamento moderado.\")\n",
        "else:\n",
        "    print(\"‚úÖ Classes relativamente equilibradas.\")\n"
      ],
      "metadata": {
        "id": "7S4nQbpuzmkJ"
      },
      "execution_count": null,
      "outputs": [],
      "id": "7S4nQbpuzmkJ"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üîß Sanitiza√ß√£o e Imputa√ß√£o s√≥ para Balanceamento (garante s√≥ num√©ricos)"
      ],
      "metadata": {
        "id": "siFCYTsCiq_6"
      },
      "id": "siFCYTsCiq_6"
    },
    {
      "cell_type": "code",
      "source": [
        "# 5.2 Sanitiza√ß√£o/Imputa√ß√£o (s√≥ para balanceamento) + Escalonamento\n",
        "# ------------------------------------------------------------------\n",
        "# - Objetivo: garantir que X esteja 100% num√©rico e sem NaN antes do Tomek/SMOTE.\n",
        "# - Passos:\n",
        "#   1) Selecionar apenas colunas num√©ricas (df_encoded j√° tende a ser num√©rico ap√≥s OneHot).\n",
        "#   2) Imputar com mediana (robusta a outliers) ‚Äî SOMENTE para esta etapa t√©cnica.\n",
        "#   3) Padronizar (StandardScaler) para que SMOTE/Tomek usem dist√¢ncias sem vi√©s de escala.\n",
        "# - Observa√ß√£o: Na modelagem final (cv/grid search), faremos TUDO em pipeline para evitar vazamento.\n",
        "\n",
        "#import numpy as np\n",
        "#from sklearn.impute import SimpleImputer\n",
        "#from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# 1) Garante num√©ricos\n",
        "X_num = X.select_dtypes(include=[np.number]).copy()\n",
        "drop_cols = [c for c in X.columns if c not in X_num.columns]\n",
        "if drop_cols:\n",
        "    print(f\"[5.2] Removidas do X (n√£o num√©ricas para balanceamento): {drop_cols}\")\n",
        "\n",
        "# 2) Imputa√ß√£o simples (mediana)\n",
        "imp_bal = SimpleImputer(strategy=\"median\")\n",
        "X_num_imp = pd.DataFrame(\n",
        "    imp_bal.fit_transform(X_num),\n",
        "    columns=X_num.columns,\n",
        "    index=X_num.index\n",
        ")\n",
        "\n",
        "# 3) Escalonamento (StandardScaler)\n",
        "scaler_bal = StandardScaler()\n",
        "X_num_imp_scaled = pd.DataFrame(\n",
        "    scaler_bal.fit_transform(X_num_imp),\n",
        "    columns=X_num.columns,\n",
        "    index=X_num.index\n",
        ")\n",
        "\n",
        "print(\"[5.2] X_num_imp_scaled pronto para Tomek/SMOTE:\", X_num_imp_scaled.shape)\n",
        "\n",
        "# (Opcional) Salvar artefatos t√©cnicos desta etapa\n",
        "# X_num_imp_scaled.to_csv(\"X_balance_ready.csv\", index=False)\n",
        "# y.to_csv(\"y_balance_ready.csv\", index=False)\n"
      ],
      "metadata": {
        "id": "15p4qR1Z2Ux4"
      },
      "execution_count": null,
      "outputs": [],
      "id": "15p4qR1Z2Ux4"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Tomek Links (remo√ß√£o de pares fronteira)**"
      ],
      "metadata": {
        "id": "1_MoefY9dKte"
      },
      "id": "1_MoefY9dKte"
    },
    {
      "cell_type": "code",
      "source": [
        "# 5.3 Tomek Links\n",
        "# ----------------\n",
        "# - Objetivo: remover pares (maioria/minoria) que formam \"bordas\" ruidosas, deixando as classes mais separ√°veis.\n",
        "# - Deve rodar AP√ìS escalonamento (dist√¢ncias ficam corretas).\n",
        "# - Resultado: base levemente menor e menos ru√≠do.\n",
        "\n",
        "#from imblearn.under_sampling import TomekLinks\n",
        "\n",
        "tl = TomekLinks(n_jobs=-1)\n",
        "X_tomek, y_tomek = tl.fit_resample(X_num_imp_scaled, y)\n",
        "\n",
        "dist_tomek = (y_tomek.value_counts(normalize=True) * 100).round(2).to_dict()\n",
        "print(\"[5.3] P√≥s-Tomek ‚Äî distribui√ß√£o (%):\", dist_tomek)\n",
        "print(\"[5.3] Shape:\", X_tomek.shape)\n"
      ],
      "metadata": {
        "id": "ITmvV0PHYezT"
      },
      "execution_count": null,
      "outputs": [],
      "id": "ITmvV0PHYezT"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **SMOTE (_Oversampling_ da Minorit√°ria)**\n"
      ],
      "metadata": {
        "id": "4CFPLm7bdwB3"
      },
      "id": "4CFPLm7bdwB3"
    },
    {
      "cell_type": "code",
      "source": [
        "# 5.4 Balanceamento com SMOTE (Synthetic Minority Oversampling Technique)\n",
        "# ----------------------------------------------------------------------\n",
        "# Objetivo: gerar amostras sint√©ticas da classe minorit√°ria para reduzir desbalanceamento.\n",
        "# Sequ√™ncia recomendada: aplicar ap√≥s Tomek Links (limpa ru√≠dos/pares conflitantes),\n",
        "#                        mas SMOTE tamb√©m pode ser usado isoladamente.\n",
        "# Param k_neighbors: n¬∫ de vizinhos usados para gerar pontos sint√©ticos.\n",
        "#   - Ajuste din√¢mico: k = min(5, minority_count - 1) ‚Üí evita erro quando minoria tem poucos registros.\n",
        "# Param random_state: garante reprodutibilidade.\n",
        "# Obs: SMOTE n√£o aceita 'n_jobs' (n√£o paraleliza internamente).\n",
        "\n",
        "#from imblearn.over_sampling import SMOTE\n",
        "\n",
        "minority_count = y_tomek.value_counts().min()\n",
        "k = min(5, max(1, minority_count - 1))\n",
        "\n",
        "smote = SMOTE(k_neighbors=k, random_state=42)\n",
        "X_tomek_smote, y_tomek_smote = smote.fit_resample(X_tomek, y_tomek)\n",
        "\n",
        "dist_tomek_smote = (y_tomek_smote.value_counts(normalize=True) * 100).round(2).to_dict()\n",
        "print(\"[5.4] P√≥s-Tomek/SMOTE ‚Äî distribui√ß√£o (%):\", dist_tomek_smote)\n",
        "print(\"[5.4] Shape:\", X_tomek_smote.shape)\n"
      ],
      "metadata": {
        "id": "qW69IJ1tYeoT"
      },
      "execution_count": null,
      "outputs": [],
      "id": "qW69IJ1tYeoT"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üíæ Salvamento de Bases Balanceadas\n"
      ],
      "metadata": {
        "id": "iE7QyX4z5m-k"
      },
      "id": "iE7QyX4z5m-k"
    },
    {
      "cell_type": "code",
      "source": [
        "# (Opcional) salvar bases balanceadas\n",
        "pd.DataFrame(X_tomek, columns=X_num_imp_scaled.columns).to_csv(\"X_mpox_2022_target_hosp_tomek.csv\", index=False)\n",
        "y_tomek.to_csv(\"y_mpox_2022_target_hosp_tomek.csv\", index=False)\n",
        "pd.DataFrame(X_tomek_smote, columns=X_num_imp_scaled.columns).to_csv(\"X_mpox_2022_target_hosp_tomek_smote.csv\", index=False)\n",
        "y_tomek_smote.to_csv(\"y_mpox_2022_target_hosp_tomek_smote.csv\", index=False)"
      ],
      "metadata": {
        "id": "m2Oepf5S5ku3"
      },
      "execution_count": null,
      "outputs": [],
      "id": "m2Oepf5S5ku3"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Auditoria comparativa de balanceamento: Antes ‚Üí Tomek ‚Üí SMOTE (Ap√≥s Tomek)"
      ],
      "metadata": {
        "id": "1GUob4FCeHe7"
      },
      "id": "1GUob4FCeHe7"
    },
    {
      "cell_type": "code",
      "source": [
        "# 5.5 Auditoria comparativa (Antes ‚Üí Tomek ‚Üí SMOTE (Ap√≥s Tomek))\n",
        "# ------------------------------------------------\n",
        "# - Objetivo: comparar a distribui√ß√£o das classes ao longo das etapas de balanceamento.\n",
        "# ------------------------------------------------\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "\n",
        "dist_original = (y.value_counts(normalize=True) * 100).round(2).to_dict()\n",
        "dist_tomek    = (y_tomek.value_counts(normalize=True) * 100).round(2).to_dict()\n",
        "dist_tomek_smote    = (y_tomek_smote.value_counts(normalize=True) * 100).round(2).to_dict()\n",
        "\n",
        "audit_balance = pd.DataFrame({\n",
        "    \"Antes (%)\": dist_original,\n",
        "    \"Ap√≥s Tomek (%)\": dist_tomek,\n",
        "    \"Ap√≥s Tomek/SMOTE (%)\": dist_tomek_smote\n",
        "}).T\n",
        "\n",
        "print(\"=== Auditoria comparativa do balanceamento ===\")\n",
        "print(audit_balance)\n",
        "\n",
        "fig, axes = plt.subplots(1, 3, figsize=(15, 5), sharey=True)\n",
        "\n",
        "sns.barplot(x=list(dist_original.keys()), y=list(dist_original.values()),\n",
        "            ax=axes[0], color=\"lightblue\", edgecolor=\"gray\")\n",
        "axes[0].set_title(\"Antes\")\n",
        "\n",
        "sns.barplot(x=list(dist_tomek.keys()), y=list(dist_tomek.values()),\n",
        "            ax=axes[1], color=\"lightgreen\", edgecolor=\"gray\")\n",
        "axes[1].set_title(\"Ap√≥s Tomek\")\n",
        "\n",
        "sns.barplot(x=list(dist_tomek_smote.keys()), y=list(dist_tomek_smote.values()),\n",
        "            ax=axes[2], color=\"green\", edgecolor=\"gray\")\n",
        "axes[2].set_title(\"Ap√≥s Tomek/SMOTE\")\n",
        "\n",
        "for ax in axes:\n",
        "    ax.set_xlabel(\"Classe\")\n",
        "    ax.set_ylabel(\"Percentual (%)\")\n",
        "\n",
        "plt.suptitle(f\"Compara√ß√£o de distribui√ß√£o ‚Äî alvo: {target_col}\", fontsize=13)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "_9bOz-i24D0g"
      },
      "execution_count": null,
      "outputs": [],
      "id": "_9bOz-i24D0g"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üíæ Serializa√ß√£o dos dados balanceados (SMOTE)"
      ],
      "metadata": {
        "id": "3egZi21PLoIB"
      },
      "id": "3egZi21PLoIB"
    },
    {
      "cell_type": "code",
      "source": [
        "# üíæ Serializa√ß√£o em Pickle (PKL) ‚Äî P√≥s-SMOTE\n",
        "# -------------------------------------------------\n",
        "# Objetivo:\n",
        "#   - Salvar os datasets balanceados em formato .pkl\n",
        "#   - Preserva dtypes (ex: Int64, categorias) que podem se perder no CSV\n",
        "# Sa√≠da:\n",
        "#   - X_mpox_2022_target_hosp_tomek_smote.pkl\n",
        "#   - y_mpox_2022_target_hosp_tomek_smote.pkl\n",
        "# -------------------------------------------------\n",
        "\n",
        "import joblib\n",
        "\n",
        "# Caminhos de sa√≠da\n",
        "X_tomek_smote_pkl = \"X_mpox_2022_target_hosp_tomek_smote.pkl\"\n",
        "y_tomek_smote_pkl = \"y_mpox_2022_target_hosp_tomek_smote.pkl\"\n",
        "\n",
        "# Serializa com joblib (mais eficiente para DataFrames grandes)\n",
        "joblib.dump(pd.DataFrame(X_tomek_smote, columns=X_num_imp_scaled.columns), X_tomek_smote_pkl)\n",
        "joblib.dump(y_tomek_smote, y_tomek_smote_pkl)\n",
        "\n",
        "print(f\"‚úîÔ∏è X_tomek_smote salvo em: {X_tomek_smote_pkl}\")\n",
        "print(f\"‚úîÔ∏è y_tomek_smote salvo em: {y_tomek_smote_pkl}\")\n",
        "\n",
        "# Checagem p√≥s-salvamento\n",
        "X_check = joblib.load(X_tomek_smote_pkl)\n",
        "y_check = joblib.load(y_tomek_smote_pkl)\n",
        "\n",
        "print(\"\\nChecagem p√≥s-salvamento PKL:\")\n",
        "print(\"X_tomek_smote:\", X_check.shape, \"‚Üí dtypes:\", X_check.dtypes.unique())\n",
        "print(\"y_tomek_smote:\", y_check.shape, \"‚Üí dtype:\", y_check.dtypes)\n",
        "print(f\"\\nDistribui√ß√£o alvo ({y_tomek_smote_pkl[0:-4]}):\")\n",
        "print(\"üéØVari√°vel-alvo:\", )\n",
        "print(y_check.value_counts(normalize=True) * 100)\n"
      ],
      "metadata": {
        "id": "_2sXVKEDLlKh"
      },
      "execution_count": null,
      "outputs": [],
      "id": "_2sXVKEDLlKh"
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## **Atividade 6: Treinamento dos modelos de Machine Learning**\n",
        "- Comparar modelos: testar diferentes classificadores\n",
        "    - Classifica√ß√£o bin√°ria\n",
        "    - Modelos: Naive Bayes, √Årvore de Decis√£o, Random Forest, Aprendizagem baseada em inst√¢ncias - kNN, Regress√£o log√≠stica, SVM, Redes Neurais Artificiais\n",
        "- Medir desempenho com m√©tricas adequadas e evitar overfitting\n",
        "- Investigar como t√©cnicas de imputa√ß√£o e codifica√ß√£o impactam os resultados\n",
        "- Escolher e ajustar modelos (gridsearch/valida√ß√£o cruzada)\n",
        "---"
      ],
      "metadata": {
        "id": "mmBvj2PyShAA"
      },
      "id": "mmBvj2PyShAA"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Configura√ß√£o Inicial:\n",
        "\n",
        "- üì¶ Importa√ß√£o das bibliotecas\n",
        "- üîÑ Carregar dados j√° pr√©-processados (Tomek + SMOTE aplicados)\n",
        "- ‚úÇÔ∏è Divis√£o treino/teste\n"
      ],
      "metadata": {
        "id": "5wKu0RZ2jYVt"
      },
      "id": "5wKu0RZ2jYVt"
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================\n",
        "# üì¶ Importa√ß√£o das bibliotecas\n",
        "# ================================\n",
        "# pandas/numpy ‚Üí manipula√ß√£o de dados\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# sklearn ‚Üí modelagem e avalia√ß√£o\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, precision_score, recall_score, f1_score, roc_auc_score,\n",
        "    classification_report, confusion_matrix, RocCurveDisplay\n",
        ")\n",
        "\n",
        "# visualiza√ß√£o\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# salvar/carregar modelos\n",
        "import joblib\n",
        "\n",
        "# ================================\n",
        "# üîÑ Carregar dados j√° pr√©-processados (Tomek + SMOTE aplicados)\n",
        "# ================================\n",
        "X = joblib.load(\"/content/X_mpox_2022_target_hosp_tomek_smote.pkl\")\n",
        "y = joblib.load(\"/content/y_mpox_2022_target_hosp_tomek_smote.pkl\")\n",
        "\n",
        "# ================================\n",
        "# ‚úÇÔ∏è Divis√£o treino/teste\n",
        "# - Mantemos propor√ß√£o de classes (stratify=y)\n",
        "# - 80% treino, 20% teste\n",
        "# ================================\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, stratify=y, random_state=42\n",
        ")\n",
        "\n",
        "print(\"Tamanho treino:\", X_train.shape, \" | Tamanho teste:\", X_test.shape)\n",
        "print(\"Distribui√ß√£o classes (treino):\\n\", y_train.value_counts(normalize=True))\n"
      ],
      "metadata": {
        "id": "7PJ77tJ2gI5e"
      },
      "id": "7PJ77tJ2gI5e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "dtA91WQBvMYs"
      },
      "id": "dtA91WQBvMYs",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fun√ß√£o auxiliar de avalia√ß√£o"
      ],
      "metadata": {
        "id": "Hxp2AECcg3tc"
      },
      "id": "Hxp2AECcg3tc"
    },
    {
      "cell_type": "code",
      "source": [
        "def avaliar_modelo(modelo, X_train, y_train, X_test, y_test, nome_modelo):\n",
        "    \"\"\"\n",
        "    Treina e avalia um modelo de classifica√ß√£o.\n",
        "\n",
        "    Passos:\n",
        "    1. Treina com dados de treino.\n",
        "    2. Prediz nos dados de teste.\n",
        "    3. Calcula m√©tricas (Accuracy, Precision, Recall, F1, AUC).\n",
        "    4. Exibe relat√≥rio, matriz de confus√£o e curva ROC.\n",
        "\n",
        "    Retorna: dicion√°rio com m√©tricas principais (para compara√ß√£o entre modelos).\n",
        "    \"\"\"\n",
        "    # Treinamento do modelo\n",
        "    modelo.fit(X_train, y_train)\n",
        "\n",
        "    # Predi√ß√£o\n",
        "    y_pred = modelo.predict(X_test)\n",
        "    y_proba = modelo.predict_proba(X_test)[:,1] if hasattr(modelo, \"predict_proba\") else None\n",
        "\n",
        "    # C√°lculo das m√©tricas\n",
        "    acc = accuracy_score(y_test, y_pred)\n",
        "    prec = precision_score(y_test, y_pred)\n",
        "    rec = recall_score(y_test, y_pred)\n",
        "    f1 = f1_score(y_test, y_pred)\n",
        "    auc = roc_auc_score(y_test, y_proba) if y_proba is not None else np.nan\n",
        "\n",
        "    # Exibi√ß√£o de resultados\n",
        "    print(f\"\\nüìä Resultados - {nome_modelo}\")\n",
        "    print(classification_report(y_test, y_pred))\n",
        "    print(\"Matriz de Confus√£o:\\n\", confusion_matrix(y_test, y_pred))\n",
        "    print(f\"AUC: {auc:.3f}\")\n",
        "\n",
        "    # Curva ROC (quando dispon√≠vel)\n",
        "    if y_proba is not None:\n",
        "        RocCurveDisplay.from_estimator(modelo, X_test, y_test)\n",
        "        plt.title(f\"Curva ROC - {nome_modelo}\")\n",
        "        plt.show()\n",
        "\n",
        "    # Retorno para compara√ß√£o final\n",
        "    return {\"Modelo\": nome_modelo, \"Accuracy\": acc, \"Precision\": prec,\n",
        "            \"Recall\": rec, \"F1\": f1, \"AUC\": auc}\n",
        "\n",
        "# Lista para armazenar os resultados de todos os modelos\n",
        "resultados = []\n"
      ],
      "metadata": {
        "id": "itHAoB5WgQe1"
      },
      "id": "itHAoB5WgQe1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualiza√ß√£o r√°pida\n",
        "print(X.head(1))\n",
        "print(y.value_counts())\n",
        "\n",
        "#type(X_loaded)\n",
        "#type(y_loaded)\n"
      ],
      "metadata": {
        "id": "AZm9Ms4apDcL"
      },
      "execution_count": null,
      "outputs": [],
      "id": "AZm9Ms4apDcL"
    },
    {
      "cell_type": "markdown",
      "source": [
        "üîπ 1. Naive Bayes\n",
        "\n",
        "üìò Teoria: assume independ√™ncia condicional entre atributos (Kelleher, cap. 8).\n",
        "Bom para baseline r√°pido, mesmo que a suposi√ß√£o seja simplista."
      ],
      "metadata": {
        "id": "1kaU9cahhA9B"
      },
      "id": "1kaU9cahhA9B"
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.naive_bayes import GaussianNB\n",
        "\n",
        "nb = GaussianNB()\n",
        "resultados.append(avaliar_modelo(nb, X_train, y_train, X_test, y_test, \"Naive Bayes\"))\n"
      ],
      "metadata": {
        "id": "z80Grh2vgYxs"
      },
      "id": "z80Grh2vgYxs",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "üîπ 2. Regress√£o Log√≠stica\n",
        "\n",
        "üìò Teoria: modelo linear que estima probabilidades. Muito usado em sa√∫de pela interpretabilidade (odds ratio)."
      ],
      "metadata": {
        "id": "_z9pZssuhMbS"
      },
      "id": "_z9pZssuhMbS"
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "log_reg = LogisticRegression(max_iter=1000, random_state=42)\n",
        "resultados.append(avaliar_modelo(log_reg, X_train, y_train, X_test, y_test, \"Regress√£o Log√≠stica\"))\n"
      ],
      "metadata": {
        "id": "LeoQGC8HhF3L"
      },
      "id": "LeoQGC8HhF3L",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "üîπ 3. √Årvore de Decis√£o\n",
        "\n",
        "üìò Teoria: gera regras hier√°rquicas ‚Äúif-then‚Äù. Boa interpretabilidade, mas risco de overfitting se n√£o podada."
      ],
      "metadata": {
        "id": "UbN7UCblhWdi"
      },
      "id": "UbN7UCblhWdi"
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "tree = DecisionTreeClassifier(random_state=42)\n",
        "resultados.append(avaliar_modelo(tree, X_train, y_train, X_test, y_test, \"√Årvore de Decis√£o\"))\n"
      ],
      "metadata": {
        "id": "n6R_Yha-hRIf"
      },
      "id": "n6R_Yha-hRIf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "üîπ 4. kNN\n",
        "\n",
        "üìò Teoria: classifica pelo ‚Äúvoto‚Äù dos vizinhos mais pr√≥ximos. Simples, mas sens√≠vel a escala e dados ruidosos"
      ],
      "metadata": {
        "id": "pc6AvDUthc8E"
      },
      "id": "pc6AvDUthc8E"
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "knn = KNeighborsClassifier(n_neighbors=5)  # k=5 √© default, pode ser ajustado\n",
        "resultados.append(avaliar_modelo(knn, X_train, y_train, X_test, y_test, \"kNN\"))\n"
      ],
      "metadata": {
        "id": "pf9v5skIhcV_"
      },
      "id": "pf9v5skIhcV_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "üîπ 5. Random Forest\n",
        "\n",
        "üìò Teoria: conjunto de v√°rias √°rvores de decis√£o com amostragem aleat√≥ria. Reduz overfitting, aumenta robustez"
      ],
      "metadata": {
        "id": "YwXSJu7YhmE_"
      },
      "id": "YwXSJu7YhmE_"
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "rf = RandomForestClassifier(n_estimators=200, random_state=42)\n",
        "resultados.append(avaliar_modelo(rf, X_train, y_train, X_test, y_test, \"Random Forest\"))\n"
      ],
      "metadata": {
        "id": "kR8gfULthis3"
      },
      "id": "kR8gfULthis3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "üîπ 6. SVM\n",
        "\n",
        "üìò Teoria: encontra hiperplano √≥timo para separar classes. Potente em dados complexos, mas exige tuning (kernel, C, Œ≥)."
      ],
      "metadata": {
        "id": "FX5EUoWYhxJ8"
      },
      "id": "FX5EUoWYhxJ8"
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import SVC\n",
        "\n",
        "svm = SVC(probability=True, random_state=42)  # probability=True para permitir ROC\n",
        "resultados.append(avaliar_modelo(svm, X_train, y_train, X_test, y_test, \"SVM\"))\n"
      ],
      "metadata": {
        "id": "6ZPmU4xZhotm"
      },
      "id": "6ZPmU4xZhotm",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "üîπ 7. Rede Neural (MLP)\n",
        "\n",
        "üìò Teoria: modelo inspirado no c√©rebro humano, com m√∫ltiplas camadas ocultas. Captura padr√µes complexos, mas menos interpret√°vel."
      ],
      "metadata": {
        "id": "HzjZeB-Hh1yJ"
      },
      "id": "HzjZeB-Hh1yJ"
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neural_network import MLPClassifier\n",
        "\n",
        "mlp = MLPClassifier(hidden_layer_sizes=(50,50), max_iter=500, random_state=42)\n",
        "resultados.append(avaliar_modelo(mlp, X_train, y_train, X_test, y_test, \"Rede Neural (MLP)\"))\n"
      ],
      "metadata": {
        "id": "lgJloFZth1Pv"
      },
      "id": "lgJloFZth1Pv",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "üîπ Compara√ß√£o final entre modelos"
      ],
      "metadata": {
        "id": "q9aFuevBh-Py"
      },
      "id": "q9aFuevBh-Py"
    },
    {
      "cell_type": "code",
      "source": [
        "# Criar DataFrame comparativo\n",
        "df_resultados = pd.DataFrame(resultados)\n",
        "\n",
        "print(\"\\nüìä Compara√ß√£o de M√©tricas entre Modelos\")\n",
        "display(df_resultados.sort_values(by=\"F1\", ascending=False))\n"
      ],
      "metadata": {
        "id": "MFSbXC9Hh7C7"
      },
      "id": "MFSbXC9Hh7C7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## **Atividade 7: Usar o modelo treinado**\n",
        "- Salvar os transformadores\n",
        "- Salvar o modelo\n",
        "- Fazer predi√ß√£o usando dados novos\n",
        "---\n"
      ],
      "metadata": {
        "id": "BdrXHnftSUui"
      },
      "id": "BdrXHnftSUui"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Naive Bayes"
      ],
      "metadata": {
        "id": "jWn_51unn13_"
      },
      "id": "jWn_51unn13_"
    },
    {
      "cell_type": "code",
      "source": [
        "# ‚úÖ Exemplo: supondo que Random Forest foi o melhor (ajustar conforme resultados)\n",
        "modelo_final = nb\n",
        "\n",
        "# Salvar pipeline completo (inclui pr√©-processamentos e modelo final)\n",
        "joblib.dump(modelo_final, \"modelo_final_nb.pkl\")\n",
        "\n",
        "# Recarregar modelo para validar reutiliza√ß√£o\n",
        "modelo_carregado = joblib.load(\"modelo_final_nb.pkl\")\n",
        "\n",
        "# Simular predi√ß√£o em 5 registros de teste\n",
        "amostra = X_test.iloc[:5]\n",
        "print(\"\\nüîÆ Predi√ß√µes em novos dados simulados:\")\n",
        "print(\"Probabilidades:\", modelo_carregado.predict_proba(amostra))\n",
        "print(\"Classe prevista:\", modelo_carregado.predict(amostra))\n"
      ],
      "metadata": {
        "id": "2aC-8x7vn5LC"
      },
      "id": "2aC-8x7vn5LC",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "SVM"
      ],
      "metadata": {
        "id": "mBgAESPkoY_H"
      },
      "id": "mBgAESPkoY_H"
    },
    {
      "cell_type": "code",
      "source": [
        "# ‚úÖ Exemplo: supondo que Random Forest foi o melhor (ajustar conforme resultados)\n",
        "modelo_final = nb\n",
        "\n",
        "# Salvar pipeline completo (inclui pr√©-processamentos e modelo final)\n",
        "joblib.dump(modelo_final, \"modelo_final_nb.pkl\")\n",
        "\n",
        "# Recarregar modelo para validar reutiliza√ß√£o\n",
        "modelo_carregado = joblib.load(\"modelo_final_nb.pkl\")\n",
        "\n",
        "# Simular predi√ß√£o em 5 registros de teste\n",
        "amostra = X_test.iloc[:5]\n",
        "print(\"\\nüîÆ Predi√ß√µes em novos dados simulados:\")\n",
        "print(\"Probabilidades:\", modelo_carregado.predict_proba(amostra))\n",
        "print(\"Classe prevista:\", modelo_carregado.predict(amostra))\n"
      ],
      "metadata": {
        "id": "_AZWmPT5oYZj"
      },
      "id": "_AZWmPT5oYZj",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "√Årvore de Decis√£o"
      ],
      "metadata": {
        "id": "qChqQlbRoenW"
      },
      "id": "qChqQlbRoenW"
    },
    {
      "cell_type": "code",
      "source": [
        "# ‚úÖ Exemplo: supondo que Random Forest foi o melhor (ajustar conforme resultados)\n",
        "modelo_final = tree\n",
        "\n",
        "# Salvar pipeline completo (inclui pr√©-processamentos e modelo final)\n",
        "joblib.dump(modelo_final, \"modelo_final_nb.pkl\")\n",
        "\n",
        "# Recarregar modelo para validar reutiliza√ß√£o\n",
        "modelo_carregado = joblib.load(\"modelo_final_nb.pkl\")\n",
        "\n",
        "# Simular predi√ß√£o em 5 registros de teste\n",
        "amostra = X_test.iloc[:5]\n",
        "print(\"\\nüîÆ Predi√ß√µes em novos dados simulados:\")\n",
        "print(\"Probabilidades:\", modelo_carregado.predict_proba(amostra))\n",
        "print(\"Classe prevista:\", modelo_carregado.predict(amostra))\n"
      ],
      "metadata": {
        "id": "sS0pOseDoiKI"
      },
      "id": "sS0pOseDoiKI",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Rede Neural (MLP)"
      ],
      "metadata": {
        "id": "Hi9YlpCPoqlJ"
      },
      "id": "Hi9YlpCPoqlJ"
    },
    {
      "cell_type": "code",
      "source": [
        "# ‚úÖ Exemplo: supondo que Random Forest foi o melhor (ajustar conforme resultados)\n",
        "modelo_final = mlp\n",
        "\n",
        "# Salvar pipeline completo (inclui pr√©-processamentos e modelo final)\n",
        "joblib.dump(modelo_final, \"modelo_final_mlp.pkl\")\n",
        "\n",
        "# Recarregar modelo para validar reutiliza√ß√£o\n",
        "modelo_carregado = joblib.load(\"modelo_final_mlp.pkl\")\n",
        "\n",
        "# Simular predi√ß√£o em 5 registros de teste\n",
        "amostra = X_test.iloc[:5]\n",
        "print(\"\\nüîÆ Predi√ß√µes em novos dados simulados:\")\n",
        "print(\"Probabilidades:\", modelo_carregado.predict_proba(amostra))\n",
        "print(\"Classe prevista:\", modelo_carregado.predict(amostra))\n"
      ],
      "metadata": {
        "id": "-Y6rw7_0op1T"
      },
      "id": "-Y6rw7_0op1T",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Random Forest"
      ],
      "metadata": {
        "id": "MgG-IbOonBsQ"
      },
      "id": "MgG-IbOonBsQ"
    },
    {
      "cell_type": "code",
      "source": [
        "# ‚úÖ Exemplo: supondo que Random Forest foi o melhor (ajustar conforme resultados)\n",
        "modelo_final = rf\n",
        "\n",
        "# Salvar pipeline completo (inclui pr√©-processamentos e modelo final)\n",
        "joblib.dump(modelo_final, \"modelo_final_rf.pkl\")\n",
        "\n",
        "# Recarregar modelo para validar reutiliza√ß√£o\n",
        "modelo_carregado = joblib.load(\"modelo_final_rf.pkl\")\n",
        "\n",
        "# Simular predi√ß√£o em 5 registros de teste\n",
        "amostra = X_test.iloc[:5]\n",
        "print(\"\\nüîÆ Predi√ß√µes em novos dados simulados:\")\n",
        "print(\"Probabilidades:\", modelo_carregado.predict_proba(amostra))\n",
        "print(\"Classe prevista:\", modelo_carregado.predict(amostra))\n"
      ],
      "metadata": {
        "id": "e5YSzvyMiHv4"
      },
      "id": "e5YSzvyMiHv4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Regress√£o Log√≠stica"
      ],
      "metadata": {
        "id": "LOSaXP4QozYq"
      },
      "id": "LOSaXP4QozYq"
    },
    {
      "cell_type": "code",
      "source": [
        "# ‚úÖ Exemplo: supondo que Random Forest foi o melhor (ajustar conforme resultados)\n",
        "modelo_final = log_reg\n",
        "\n",
        "# Salvar pipeline completo (inclui pr√©-processamentos e modelo final)\n",
        "joblib.dump(modelo_final, \"modelo_final_log_reg.pkl\")\n",
        "\n",
        "# Recarregar modelo para validar reutiliza√ß√£o\n",
        "modelo_carregado = joblib.load(\"modelo_final_log_reg.pkl\")\n",
        "\n",
        "# Simular predi√ß√£o em 5 registros de teste\n",
        "amostra = X_test.iloc[:5]\n",
        "print(\"\\nüîÆ Predi√ß√µes em novos dados simulados:\")\n",
        "print(\"Probabilidades:\", modelo_carregado.predict_proba(amostra))\n",
        "print(\"Classe prevista:\", modelo_carregado.predict(amostra))\n"
      ],
      "metadata": {
        "id": "sUahknWao0WU"
      },
      "id": "sUahknWao0WU",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "kNN"
      ],
      "metadata": {
        "id": "9LwP_6F_pB43"
      },
      "id": "9LwP_6F_pB43"
    },
    {
      "cell_type": "code",
      "source": [
        "# ‚úÖ Exemplo: supondo que Random Forest foi o melhor (ajustar conforme resultados)\n",
        "modelo_final = knn\n",
        "\n",
        "# Salvar pipeline completo (inclui pr√©-processamentos e modelo final)\n",
        "joblib.dump(modelo_final, \"modelo_final_knn.pkl\")\n",
        "\n",
        "# Recarregar modelo para validar reutiliza√ß√£o\n",
        "modelo_carregado = joblib.load(\"modelo_final_knn.pkl\")\n",
        "\n",
        "# Simular predi√ß√£o em 5 registros de teste\n",
        "amostra = X_test.iloc[:5]\n",
        "print(\"\\nüîÆ Predi√ß√µes em novos dados simulados:\")\n",
        "print(\"Probabilidades:\", modelo_carregado.predict_proba(amostra))\n",
        "print(\"Classe prevista:\", modelo_carregado.predict(amostra))\n"
      ],
      "metadata": {
        "id": "RgEe8QGLpA-p"
      },
      "id": "RgEe8QGLpA-p",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# ================================\n",
        "# üìÇ Criar pasta para salvar os modelos\n",
        "# ================================\n",
        "os.makedirs(\"modelos_salvos\", exist_ok=True)\n",
        "\n",
        "# ================================\n",
        "# üîÑ Loop para salvar e recarregar todos os modelos\n",
        "# ================================\n",
        "modelos_treinados = {\n",
        "    \"NaiveBayes\": nb,\n",
        "    \"RegressaoLogistica\": log_reg,\n",
        "    \"ArvoreDecisao\": tree,\n",
        "    \"kNN\": knn,\n",
        "    \"RandomForest\": rf,\n",
        "    \"SVM\": svm,\n",
        "    \"RedeNeuralMLP\": mlp\n",
        "}\n",
        "\n",
        "for nome, modelo in modelos_treinados.items():\n",
        "    print(f\"\\n=== üîÑ Testando reuso do modelo: {nome} ===\")\n",
        "\n",
        "    # 1. Salvar modelo em arquivo PKL\n",
        "    caminho = f\"modelos_salvos/{nome}.pkl\"\n",
        "    joblib.dump(modelo, caminho)\n",
        "    print(f\"‚úÖ Modelo salvo em: {caminho}\")\n",
        "\n",
        "    # 2. Recarregar modelo salvo\n",
        "    modelo_carregado = joblib.load(caminho)\n",
        "\n",
        "    # 3. Testar predi√ß√£o em uma pequena amostra (5 registros de teste)\n",
        "    amostra = X_test.iloc[:5]\n",
        "    pred = modelo_carregado.predict(amostra)\n",
        "    proba = modelo_carregado.predict_proba(amostra)[:,1] if hasattr(modelo_carregado, \"predict_proba\") else \"N/A\"\n",
        "\n",
        "    print(\"üîÆ Predi√ß√µes:\", pred)\n",
        "    print(\"üìä Probabilidades:\", proba)\n"
      ],
      "metadata": {
        "id": "uQLxoccLp83m"
      },
      "id": "uQLxoccLp83m",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bs58HZ6a5Ptq"
      },
      "id": "bs58HZ6a5Ptq",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.3"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}